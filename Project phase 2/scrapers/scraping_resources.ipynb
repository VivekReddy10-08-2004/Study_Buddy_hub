{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cghPxS76d4HQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "StudyBuddy Hub Resource Scraping Notebook\n",
        "Author: Sarah Kayembe\n",
        "Course: COS 557 Database Systems (Phase II)\n",
        "Date: November 10, 2025\n",
        "\n",
        "Project Purpose\n",
        "This notebook demonstrates the process of scraping publicly available educational resources for integration into the StudyBuddy Hub system.\n",
        "The data collected will support the Course Resources module, allowing students and instructors to:\n",
        "- Access verified learning materials (articles, videos, exercises).\n",
        "- Contribute their own study resources.\n",
        "- Support gamified learning and course management features.\n",
        "\n",
        "Technologies Used\n",
        "- Python (Requests, BeautifulSoup, Pandas)\n",
        "- Khan Academy Open API / GraphQL endpoints\n",
        "- CSV output for MySQL data import\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QtnneaqzFVq",
        "outputId": "aca7c600-e4fc-46e8-8784-80cc54051f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-> Starting precise scrape on: https://www.geeksforgeeks.org/dsa/introduction-to-data-structures/\n",
            "\n",
            " Data saved successfully to ../data/Raw_data/course_resources.csv\n"
          ]
        }
      ],
      "source": [
        "# Scrapes the GeeksforGeeks \"Data Structures\" page to extract\n",
        "# topic titles, short descriptions, and resource URLs. The data is structured\n",
        "# into a CSV file for integration into the resources table.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "import os\n",
        "\n",
        "def scrape_introduction_to_ds(url: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scrapes the target GeeksforGeeks 'Introduction to Data Structures' page\n",
        "    using the precise HTML selectors provided for topic links and descriptions.\n",
        "    \"\"\"\n",
        "    print(f\"-> Starting precise scrape on: {url}\")\n",
        "\n",
        "    HEADERS = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    scraped_topics: List[Dict[str, str]] = []\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Selector Strategy\n",
        "        # Find all <a> tags that are main topic links (rel=\"noopener\" and contain bold text).\n",
        "        topic_links = soup.find_all('a', rel=\"noopener\")\n",
        "\n",
        "        # Iterate through links and find the immediate following description paragraph\n",
        "        for topic_link in topic_links:\n",
        "            # Only process links that are part of the core topic structure (contain bold text)\n",
        "            if not topic_link.find('b'):\n",
        "                continue\n",
        "\n",
        "            # Extract Topic Name and URL\n",
        "            topic_name = topic_link.get_text(strip=True)\n",
        "            resource_url = topic_link.get('href')\n",
        "\n",
        "            # Find the Description (<p dir=\"ltr\">)\n",
        "            description_snippets = []\n",
        "\n",
        "            # The description follows the link's container (either <h2> or a standalone element)\n",
        "            start_element = topic_link.parent if topic_link.parent.name == 'h2' else topic_link\n",
        "\n",
        "            sibling = start_element.next_sibling\n",
        "\n",
        "            # Find the first <p dir=\"ltr\"> sibling\n",
        "            while sibling:\n",
        "                if sibling.name == 'p' and sibling.get('dir') == 'ltr':\n",
        "                    text = sibling.get_text(strip=True)\n",
        "                    if text:\n",
        "                        description_snippets.append(text)\n",
        "                    break\n",
        "\n",
        "                # Stop if we hit the next major section marker\n",
        "                if sibling.name == 'h2':\n",
        "                    break\n",
        "\n",
        "                sibling = sibling.next_sibling\n",
        "\n",
        "            content_snippet = ' '.join(description_snippets)\n",
        "\n",
        "            if topic_name and content_snippet:\n",
        "                scraped_topics.append({\n",
        "                    'CourseName': 'Data Structures',\n",
        "                    'TopicName': topic_name,\n",
        "                    'ContentSnippet': content_snippet,\n",
        "                    'ResourceURL': resource_url,\n",
        "                    'ResourceType': 'LINK'\n",
        "                })\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\" ERROR: Failed to fetch {url}. Reason: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(scraped_topics)\n",
        "    return df\n",
        "\n",
        "# EXECUTION\n",
        "TARGET_URL = \"https://www.geeksforgeeks.org/dsa/introduction-to-data-structures/\"\n",
        "\n",
        "# Run the precise scrape function\n",
        "df_topics = scrape_introduction_to_ds(TARGET_URL)\n",
        "\n",
        "# Display the results\n",
        "if not df_topics.empty:\n",
        "    # Define output path outside the 'scrapers' folder\n",
        "    output_dir = os.path.join(\"..\", \"data\", \"Raw_data\")\n",
        "    os.makedirs(output_dir, exist_ok=True)  # ensure the directory exists\n",
        "    \n",
        "    output_filename = \"course_resources.csv\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "    # Save file to data/Raw_data/\n",
        "    df_topics.to_csv(output_path, index=False, encoding='utf-8')\n",
        "    print(f\"\\n Data saved successfully to {output_path}\")\n",
        "else:\n",
        "    print(\"\\n No topics were successfully scraped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_ieZr9Mi589",
        "outputId": "61f3c912-447c-41c6-cf0a-2d54fe09db71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-> Scraping main article with fixed selectors: https://www.geeksforgeeks.org/dsa/introduction-to-algorithms/\n",
            "-> Creating manual entry for PDF: https://www.montclair.edu/computer-science-education/wp-content/uploads/sites/253/2024/02/3-5-8.1.5-Algorithms-Programming.pdf\n",
            " Found existing file '/Users/sarahkayembe/Documents/Database_cos557/Project phase 2/data/Raw_data/course_resources.csv' with 10 entries.\n",
            "\n",
            "======================================================================\n",
            " Data successfully appended to '/Users/sarahkayembe/Documents/Database_cos557/Project phase 2/data/Raw_data/course_resources.csv'.\n",
            "Total entries now: 12\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Scrapes a GeeksforGeeks article and creates a manual PDF entry for 'Algorithms in Programming',\n",
        "# appending both to the course resources CSV.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "from typing import List, Dict\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration\n",
        "\n",
        "scrapers_dir = Path(__file__).resolve().parent if \"__file__\" in locals() else Path(os.getcwd())\n",
        "project_root = scrapers_dir.parent\n",
        "\n",
        "# Step 2: Build output directory path inside data/Raw_data/\n",
        "output_dir = project_root / \"data\" / \"Raw_data\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Step 3: Define full CSV save path\n",
        "CSV_FILENAME = \"course_resources.csv\"\n",
        "CSV_PATH = output_dir / CSV_FILENAME\n",
        "\n",
        "# Source URLs\n",
        "ALGORITHMS_GfG_URL = 'https://www.geeksforgeeks.org/dsa/introduction-to-algorithms/'\n",
        "ALGORITHMS_PDF_URL = 'https://www.montclair.edu/computer-science-education/wp-content/uploads/sites/253/2024/02/3-5-8.1.5-Algorithms-Programming.pdf'\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "COURSE_NAME = 'Algorithms in Programming'\n",
        "\n",
        "def scrape_gfg_main_article_fixed(url: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Scrapes the main title and the first descriptive paragraph using the\n",
        "    HTML structure provided by the user.\n",
        "    \"\"\"\n",
        "    print(f\"-> Scraping main article with fixed selectors: {url}\")\n",
        "\n",
        "    topic_data = {\n",
        "        'CourseName': COURSE_NAME,\n",
        "        'TopicName': 'Scrape Failed: Introduction to Algorithms',\n",
        "        'ContentSnippet': 'Scrape failed. Check selectors or URL.',\n",
        "        'ResourceURL': url,\n",
        "        'ResourceType': 'LINK'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract Main Title (TopicName) using the corrected selector\n",
        "        # Title is inside <h1> within <div class=\"article-title\">\n",
        "        title_container = soup.find('div', class_='article-title')\n",
        "        if title_container and title_container.find('h1'):\n",
        "            topic_data['TopicName'] = title_container.find('h1').get_text(strip=True)\n",
        "\n",
        "        # Extract First Descriptive Paragraph (ContentSnippet) using the corrected selector\n",
        "        # Description is inside the first <p dir=\"ltr\"> within <div class=\"text\">\n",
        "        content_container = soup.find('div', class_='text')\n",
        "\n",
        "        if content_container:\n",
        "            first_paragraph = content_container.find('p', dir='ltr')\n",
        "            if first_paragraph:\n",
        "                snippet = first_paragraph.get_text(strip=True)\n",
        "                # Clean up the snippet to look like a summary\n",
        "                cleaned_snippet = snippet.split(\" Or \")[0].replace(\"The word Algorithm means\", \"An algorithm is defined as\").replace(\"\\\"\", \"\").replace(\"'\", \"\")\n",
        "                topic_data['ContentSnippet'] = cleaned_snippet[:250] + \"...\" if len(cleaned_snippet) > 250 else cleaned_snippet\n",
        "            else:\n",
        "                topic_data['ContentSnippet'] = \"Main description paragraph not found in <div class='text'>.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" ERROR scraping {url}: {e}\")\n",
        "\n",
        "    return topic_data\n",
        "\n",
        "def create_pdf_entry(url: str) -> Dict[str, str]:\n",
        "    \"\"\"Creates a manual entry for the PDF resource.\"\"\"\n",
        "    print(f\"-> Creating manual entry for PDF: {url}\")\n",
        "\n",
        "    return {\n",
        "        'CourseName': COURSE_NAME,\n",
        "        'TopicName': 'Algorithms and Programming Concepts (PDF)',\n",
        "        'ContentSnippet': 'A comprehensive, multi-page PDF document covering core concepts in algorithms, programming, and computer science.',\n",
        "        'ResourceURL': url,\n",
        "        'ResourceType': 'PDF'\n",
        "    }\n",
        "\n",
        "def append_to_csv(new_data: List[Dict[str, str]], filename: str):\n",
        "    \"\"\"Reads existing data, appends new data, and saves back to the file.\"\"\"\n",
        "\n",
        "    df_new = pd.DataFrame(new_data)\n",
        "\n",
        "    # Read existing data, or create an empty DataFrame if file doesn't exist\n",
        "    if os.path.exists(filename):\n",
        "        df_existing = pd.read_csv(filename)\n",
        "        print(f\" Found existing file '{filename}' with {len(df_existing)} entries.\")\n",
        "    else:\n",
        "        # If the file doesn't exist (e.g., first run), create a new one with the expected columns\n",
        "        df_existing = pd.DataFrame(columns=df_new.columns)\n",
        "        print(f\" Creating new file '{filename}'.\")\n",
        "\n",
        "    # Ensure consistent columns before concatenating\n",
        "    df_existing = df_existing.reindex(columns=df_new.columns, fill_value='')\n",
        "\n",
        "    # Append new data\n",
        "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
        "\n",
        "    # Save the combined data back to the original file\n",
        "    df_combined.to_csv(filename, index=False, encoding='utf-8')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\" Data successfully appended to '{filename}'.\")\n",
        "    print(f\"Total entries now: {len(df_combined)}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "# MAIN EXECUTION\n",
        "if __name__ == \"__main__\":\n",
        "    # Gather new data\n",
        "    new_entries = []\n",
        "\n",
        "    # Scrape GfG article with fixed selectors\n",
        "    gfg_entry = scrape_gfg_main_article_fixed(ALGORITHMS_GfG_URL)\n",
        "    new_entries.append(gfg_entry)\n",
        "\n",
        "    # Create PDF entry\n",
        "    pdf_entry = create_pdf_entry(ALGORITHMS_PDF_URL)\n",
        "    new_entries.append(pdf_entry)\n",
        "\n",
        "    # Append to the existing CSV file\n",
        "    append_to_csv(new_entries, CSV_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdxCDvHJ6uuF",
        "outputId": "6e81a10c-99a4-4d4f-b74f-e50777d632a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-> Starting scrape for Web Applications Development: https://www.geeksforgeeks.org/web-tech/web-technology/\n",
            " Found existing file '/Users/sarahkayembe/Documents/Database_cos557/Project phase 2/data/Raw_data/course_resources.csv' with 12 entries.\n",
            "\n",
            "======================================================================\n",
            " Data successfully appended to '/Users/sarahkayembe/Documents/Database_cos557/Project phase 2/data/Raw_data/course_resources.csv'.\n",
            "Total entries now: 15\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Scrapes the GeeksforGeeks 'Web Technology' article, extracting the main topic\n",
        "# and sub-topics (Frontend/Backend) into separate entries, and appends the data to the course resources CSV file.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "from typing import List, Dict\n",
        "\n",
        "# Configuration\n",
        "\n",
        "scrapers_dir = Path(__file__).resolve().parent if \"__file__\" in locals() else Path(os.getcwd())\n",
        "project_root = scrapers_dir.parent\n",
        "\n",
        "# Build output directory path inside data/Raw_data/\n",
        "output_dir = project_root / \"data\" / \"Raw_data\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define full CSV save path\n",
        "CSV_FILENAME = \"course_resources.csv\"\n",
        "CSV_PATH = output_dir / CSV_FILENAME\n",
        "\n",
        "# Source URL\n",
        "WEB_TECH_URL = 'https://www.geeksforgeeks.org/web-tech/web-technology/'\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "COURSE_NAME = 'Web Applications Development'\n",
        "\n",
        "def scrape_web_tech_article(url: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Scrapes the main topic, Backend, and Frontend sections from the GeeksforGeeks Web Technology URL.\n",
        "    \"\"\"\n",
        "    new_entries = []\n",
        "\n",
        "    print(f\"-> Starting scrape for Web Applications Development: {url}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Main Topic Entry\n",
        "        # Get Main Title\n",
        "        main_title = \"Web Technology | Complete Overview\"\n",
        "        title_element = soup.find('h1', class_='main-heading') or soup.find('div', class_='article-title').find('h1')\n",
        "        if title_element:\n",
        "            main_title = title_element.get_text(strip=True)\n",
        "\n",
        "        # Get Main Description Snippet (first paragraph in the main text container)\n",
        "        content_container = soup.find('div', class_='text')\n",
        "        main_snippet = \"Definition of Web Technology not clearly found.\"\n",
        "\n",
        "        if content_container:\n",
        "            first_paragraph = content_container.find('p', dir='ltr')\n",
        "            if first_paragraph:\n",
        "                main_snippet = first_paragraph.get_text(strip=True)\n",
        "                main_snippet = main_snippet[:250].strip() + \"...\" if len(main_snippet) > 250 else main_snippet\n",
        "\n",
        "        new_entries.append({\n",
        "            'CourseName': COURSE_NAME,\n",
        "            'TopicName': main_title,\n",
        "            'ContentSnippet': main_snippet,\n",
        "            'ResourceURL': url,\n",
        "            'ResourceType': 'LINK'\n",
        "        })\n",
        "\n",
        "\n",
        "        # Backend Development Sub-Topic Entry\n",
        "        backend_h2 = soup.find('h2', id='backend-development')\n",
        "        if backend_h2:\n",
        "            backend_topic = backend_h2.get_text(strip=True).replace(':', '')\n",
        "            backend_p = backend_h2.find_next_sibling('p')\n",
        "            backend_snippet = backend_p.get_text(strip=True) if backend_p else \"Description not found for Backend Development.\"\n",
        "            backend_snippet = backend_snippet[:250].strip() + \"...\" if len(backend_snippet) > 250 else backend_snippet\n",
        "\n",
        "            new_entries.append({\n",
        "                'CourseName': COURSE_NAME,\n",
        "                'TopicName': backend_topic,\n",
        "                'ContentSnippet': backend_snippet,\n",
        "                'ResourceURL': url + \"#backend-development\",\n",
        "                'ResourceType': 'LINK'\n",
        "            })\n",
        "\n",
        "        # Frontend Development Sub-Topic Entry\n",
        "        frontend_h2 = soup.find('h2', id='frontend-development')\n",
        "        if frontend_h2:\n",
        "            frontend_topic = frontend_h2.get_text(strip=True).replace(':', '')\n",
        "            frontend_p = frontend_h2.find_next_sibling('p')\n",
        "            frontend_snippet = frontend_p.get_text(strip=True) if frontend_p else \"Description not found for Frontend Development.\"\n",
        "            frontend_snippet = frontend_snippet[:250].strip() + \"...\" if len(frontend_snippet) > 250 else frontend_snippet\n",
        "\n",
        "            new_entries.append({\n",
        "                'CourseName': COURSE_NAME,\n",
        "                'TopicName': frontend_topic,\n",
        "                'ContentSnippet': frontend_snippet,\n",
        "                'ResourceURL': url + \"#frontend-development\",\n",
        "                'ResourceType': 'LINK'\n",
        "            })\n",
        "\n",
        "        if not backend_h2 and not frontend_h2:\n",
        "            print(\" WARNING: Neither Backend nor Frontend Development sections were found.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\" ERROR: Failed to fetch {url}. Reason: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\" ERROR: An unexpected error occurred: {e}\")\n",
        "\n",
        "    return new_entries\n",
        "\n",
        "def append_to_csv(new_data: List[Dict[str, str]], filename: str):\n",
        "    \"\"\"Reads existing data, appends new data, and saves back to the file.\"\"\"\n",
        "\n",
        "    if not new_data:\n",
        "        print(\"\\n No new data scraped. File not updated.\")\n",
        "        return\n",
        "\n",
        "    df_new = pd.DataFrame(new_data)\n",
        "\n",
        "    # Define required columns to ensure consistency\n",
        "    REQUIRED_COLUMNS = ['CourseName', 'TopicName', 'ContentSnippet', 'ResourceURL', 'ResourceType']\n",
        "\n",
        "    # Read existing data, or create an empty DataFrame if file doesn't exist\n",
        "    if os.path.exists(filename):\n",
        "        df_existing = pd.read_csv(filename)\n",
        "        print(f\" Found existing file '{filename}' with {len(df_existing)} entries.\")\n",
        "    else:\n",
        "        # Create a new DataFrame with the required columns\n",
        "        df_existing = pd.DataFrame(columns=REQUIRED_COLUMNS)\n",
        "        print(f\" Creating new file '{filename}'.\")\n",
        "\n",
        "    # Ensure consistent columns before concatenating\n",
        "    df_existing = df_existing.reindex(columns=REQUIRED_COLUMNS, fill_value='')\n",
        "\n",
        "    # Append new data\n",
        "    df_combined = pd.concat([df_existing, df_new.reindex(columns=REQUIRED_COLUMNS)], ignore_index=True)\n",
        "\n",
        "    # Save the combined data back to the original file\n",
        "    df_combined.to_csv(filename, index=False, encoding='utf-8')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\" Data successfully appended to '{filename}'.\")\n",
        "    print(f\"Total entries now: {len(df_combined)}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "# MAIN EXECUTION\n",
        "if __name__ == \"__main__\":\n",
        "    # Gather new data\n",
        "    web_tech_entries = scrape_web_tech_article(WEB_TECH_URL)\n",
        "\n",
        "    # Append to the existing CSV file\n",
        "    append_to_csv(web_tech_entries, CSV_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgiv2-hloQK-",
        "outputId": "bd3930ff-4ea2-4964-b0d7-3b8c006a8903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Fetching all course links from: https://www.w3schools.com/python/python_intro.asp\n",
            " Found 232 unique tutorial pages.\n",
            "\n",
            " Starting scrape of 232 pages...\n",
            " Found existing file '/Users/sarahkayembe/Documents/Database_cos557/Project phase 2/data/Raw_data/course_resources.csv' with 15 entries.\n",
            "\n",
            "================================================================================\n",
            " Scraping complete! Data successfully appended to '/Users/sarahkayembe/Documents/Database_cos557/Project phase 2/data/Raw_data/course_resources.csv'.\n",
            "Total entries now: 247\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "## Scrapes the W3Schools Python tutorial sidebar to find all course links,\n",
        "# then scrapes each page for its title and first sentence description, appending all data to the course resources CSV.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# Configuration\n",
        "\n",
        "scrapers_dir = Path(__file__).resolve().parent if \"__file__\" in locals() else Path(os.getcwd())\n",
        "project_root = scrapers_dir.parent\n",
        "\n",
        "# Build output directory path inside data/Raw_data/\n",
        "output_dir = project_root / \"data\" / \"Raw_data\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define full CSV save path\n",
        "CSV_FILENAME = \"course_resources.csv\"\n",
        "CSV_PATH = output_dir / CSV_FILENAME\n",
        "\n",
        "# Source URLs\n",
        "START_URL = \"https://www.w3schools.com/python/python_intro.asp\"\n",
        "BASE_URL_PATH = \"https://www.w3schools.com/python/\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "COURSE_NAME = \"Python Programming\"\n",
        "REQUIRED_COLUMNS = ['CourseName', 'TopicName', 'ResourceURL', 'ContentSnippet', 'ResourceType']\n",
        "\n",
        "# Core Functions\n",
        "def get_all_course_links(start_url: str) -> List[str]:\n",
        "    \"\"\"Scrapes the starting page to find all tutorial links in the sidebar navigation.\"\"\"\n",
        "    print(f\" Fetching all course links from: {start_url}\")\n",
        "    link_urls = set()\n",
        "\n",
        "    try:\n",
        "        response = requests.get(start_url, headers=HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        nav_container = soup.find(id='leftmenuinnerinner')\n",
        "\n",
        "        if nav_container:\n",
        "            links = nav_container.find_all('a', href=True)\n",
        "            for link in links:\n",
        "                relative_url = link['href']\n",
        "                if relative_url.endswith('.asp'):\n",
        "                    full_url = urljoin(BASE_URL_PATH, relative_url)\n",
        "                    link_urls.add(full_url)\n",
        "        else:\n",
        "            print(\" ERROR: Could not find the main navigation container.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\" ERROR: Failed to fetch links from {start_url}. Reason: {e}\")\n",
        "\n",
        "    sorted_urls = sorted(list(link_urls))\n",
        "    print(f\" Found {len(sorted_urls)} unique tutorial pages.\")\n",
        "    return sorted_urls\n",
        "\n",
        "def scrape_page_details(url: str) -> Dict[str, str]:\n",
        "    \"\"\"Visits a single URL and extracts the main title and full first sentence for description.\"\"\"\n",
        "    data = {\n",
        "        'CourseName': COURSE_NAME,\n",
        "        'TopicName': \"Title Not Found\",\n",
        "        'ResourceURL': url,\n",
        "        'ContentSnippet': \"Content Not Found\",\n",
        "        'ResourceType': 'LINK'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract Topic Name (Title)\n",
        "        main_h1 = soup.find('div', id='main').find('h1')\n",
        "        if main_h1:\n",
        "            data['TopicName'] = main_h1.get_text(strip=True).split('Example')[0].split('Tutorial')[0].strip()\n",
        "\n",
        "        # Extract Description Snippet (Full Sentences)\n",
        "        main_content_div = soup.find('div', id='main')\n",
        "        if main_content_div:\n",
        "            # Find the first few non-empty paragraphs\n",
        "            first_p = main_content_div.find('p', recursive=True)\n",
        "\n",
        "            if first_p and first_p.get_text(strip=True):\n",
        "                raw_text = first_p.get_text(strip=True)\n",
        "\n",
        "                # Use a regular expression to find sentence-ending punctuation (., !, ?)\n",
        "                # followed by whitespace or end of string, and capture the text up to that point.\n",
        "                # This ensures we get a complete sentence.\n",
        "                match = re.search(r'([.!?])\\s+|\\Z', raw_text)\n",
        "\n",
        "                if match:\n",
        "                    # Capture the text up to and including the punctuation, then strip whitespace.\n",
        "                    end_index = match.end() if match.group(1) else len(raw_text)\n",
        "                    snippet = raw_text[:end_index].strip()\n",
        "                else:\n",
        "                    # If no punctuation is found, just take the raw text (e.g., if it's a very short line)\n",
        "                    snippet = raw_text.strip()\n",
        "\n",
        "                data['ContentSnippet'] = snippet[:250] + \"...\" if len(snippet) > 250 else snippet\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        data['TopicName'] = f\"ERROR: Fetch Failed\"\n",
        "        data['ContentSnippet'] = str(e)\n",
        "    except Exception as e:\n",
        "        data['TopicName'] = f\"ERROR: Scrape Failed\"\n",
        "        data['ContentSnippet'] = str(e)\n",
        "\n",
        "    return data\n",
        "\n",
        "def append_to_csv(new_data: List[Dict[str, str]], filename: str):\n",
        "    \"\"\"Reads existing data, appends new data, and saves back to the file.\"\"\"\n",
        "\n",
        "    if not new_data:\n",
        "        print(\"\\n No new data scraped. File not updated.\")\n",
        "        return\n",
        "\n",
        "    df_new = pd.DataFrame(new_data)\n",
        "\n",
        "    # Read existing data, or create an empty DataFrame if file doesn't exist\n",
        "    if os.path.exists(filename):\n",
        "        # We need to explicitly handle the columns to avoid issues if the original file structure changed\n",
        "        try:\n",
        "            df_existing = pd.read_csv(filename)\n",
        "        except Exception as e:\n",
        "            print(f\" WARNING: Failed to read existing CSV (Error: {e}). Starting a new file.\")\n",
        "            df_existing = pd.DataFrame(columns=REQUIRED_COLUMNS)\n",
        "        print(f\" Found existing file '{filename}' with {len(df_existing)} entries.\")\n",
        "    else:\n",
        "        df_existing = pd.DataFrame(columns=REQUIRED_COLUMNS)\n",
        "        print(f\" Creating new file '{filename}'.\")\n",
        "\n",
        "    # Ensure consistent columns before concatenating\n",
        "    df_existing = df_existing.reindex(columns=REQUIRED_COLUMNS, fill_value='')\n",
        "\n",
        "    # Append new data\n",
        "    df_combined = pd.concat([df_existing, df_new.reindex(columns=REQUIRED_COLUMNS)], ignore_index=True)\n",
        "\n",
        "    # Save the combined data back to the original file\n",
        "    df_combined.to_csv(filename, index=False, encoding='utf-8')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\" Scraping complete! Data successfully appended to '{filename}'.\")\n",
        "    print(f\"Total entries now: {len(df_combined)}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "# MAIN EXECUTION\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Gather new data\n",
        "    all_urls = get_all_course_links(START_URL)\n",
        "    scraped_data = []\n",
        "\n",
        "    if all_urls:\n",
        "        print(f\"\\n Starting scrape of {len(all_urls)} pages...\")\n",
        "        for i, url in enumerate(all_urls):\n",
        "            # print(f\"   -> Scraping page {i + 1}/{len(all_urls)}\")\n",
        "            details = scrape_page_details(url)\n",
        "            scraped_data.append(details)\n",
        "            time.sleep(0.1) # Politeness factor, slightly reduced for speed.\n",
        "\n",
        "    # Append to the existing CSV file\n",
        "    append_to_csv(scraped_data, CSV_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlKcbQ_s_IaS",
        "outputId": "e1878337-12f1-40c2-e61a-8b5f1c95ebd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Fetching all course links from: https://www.w3schools.com/java/default.asp\n",
            " Found 433 unique tutorial pages.\n",
            "\n",
            " Starting scrape of 433 pages...\n",
            "   -> Progress: 0/433\n",
            "   -> Progress: 20/433\n",
            "   -> Progress: 40/433\n",
            "   -> Progress: 60/433\n",
            "   -> Progress: 80/433\n",
            "   -> Progress: 100/433\n",
            "   -> Progress: 120/433\n",
            "   -> Progress: 140/433\n",
            "   -> Progress: 160/433\n",
            "   -> Progress: 180/433\n",
            "   -> Progress: 200/433\n",
            "   -> Progress: 220/433\n",
            "   -> Progress: 240/433\n",
            "   -> Progress: 260/433\n",
            "   -> Progress: 280/433\n",
            "   -> Progress: 300/433\n",
            "   -> Progress: 320/433\n",
            "   -> Progress: 340/433\n",
            "   -> Progress: 360/433\n",
            "   -> Progress: 380/433\n",
            "   -> Progress: 400/433\n",
            "   -> Progress: 420/433\n",
            " Found existing file '/Users/sarahkayembe/Documents/Database_cos557/Project phase 2/data/Raw_data/course_resources.csv' with 247 entries.\n",
            "\n",
            "================================================================================\n",
            " Scraping complete! Java data successfully appended to '/Users/sarahkayembe/Documents/Database_cos557/Project phase 2/data/Raw_data/course_resources.csv'.\n",
            "Total entries now: 680\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Scrapes the W3Schools Java tutorial sidebar to find all course links, then scrapes\n",
        "# each page for its title and first sentence description, appending all data to the course resources CSV.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# Configuration\n",
        "scrapers_dir = Path(__file__).resolve().parent if \"__file__\" in locals() else Path(os.getcwd())\n",
        "project_root = scrapers_dir.parent\n",
        "\n",
        "# Build output directory path inside data/Raw_data/\n",
        "output_dir = project_root / \"data\" / \"Raw_data\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define full CSV save path\n",
        "CSV_FILENAME = \"course_resources.csv\"\n",
        "CSV_PATH = output_dir / CSV_FILENAME\n",
        "\n",
        "# Source URLs\n",
        "START_URL = \"https://www.w3schools.com/java/default.asp\"\n",
        "BASE_URL_PATH = \"https://www.w3schools.com/java/\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "COURSE_NAME = \"Structured Problem Solving\"\n",
        "REQUIRED_COLUMNS = ['CourseName', 'TopicName', 'ResourceURL', 'ContentSnippet', 'ResourceType']\n",
        "\n",
        "# Core Functions (Same reliable logic as before)\n",
        "def get_all_course_links(start_url: str) -> List[str]:\n",
        "    \"\"\"Scrapes the starting page to find all tutorial links in the sidebar navigation.\"\"\"\n",
        "    print(f\" Fetching all course links from: {start_url}\")\n",
        "    link_urls = set()\n",
        "\n",
        "    try:\n",
        "        response = requests.get(start_url, headers=HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # W3Schools uses this consistent ID for the navigation sidebar\n",
        "        nav_container = soup.find(id='leftmenuinnerinner')\n",
        "\n",
        "        if nav_container:\n",
        "            links = nav_container.find_all('a', href=True)\n",
        "            for link in links:\n",
        "                relative_url = link['href']\n",
        "                # Java tutorial links also end in .asp\n",
        "                if relative_url.endswith('.asp'):\n",
        "                    full_url = urljoin(BASE_URL_PATH, relative_url)\n",
        "                    link_urls.add(full_url)\n",
        "        else:\n",
        "            print(\" ERROR: Could not find the main navigation container.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\" ERROR: Failed to fetch links from {start_url}. Reason: {e}\")\n",
        "\n",
        "    sorted_urls = sorted(list(link_urls))\n",
        "    print(f\" Found {len(sorted_urls)} unique tutorial pages.\")\n",
        "    return sorted_urls\n",
        "\n",
        "def scrape_page_details(url: str) -> Dict[str, str]:\n",
        "    \"\"\"Visits a single URL and extracts the main title and full first sentence for description.\"\"\"\n",
        "    data = {\n",
        "        'CourseName': COURSE_NAME,\n",
        "        'TopicName': \"Title Not Found\",\n",
        "        'ResourceURL': url,\n",
        "        'ContentSnippet': \"Content Not Found\",\n",
        "        'ResourceType': 'LINK'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract Topic Name (Title)\n",
        "        main_h1 = soup.find('div', id='main').find('h1')\n",
        "        if main_h1:\n",
        "            # Clean the title\n",
        "            data['TopicName'] = main_h1.get_text(strip=True).split('Example')[0].split('Tutorial')[0].strip()\n",
        "\n",
        "        # Extract Description Snippet (Full Sentences Logic)\n",
        "        main_content_div = soup.find('div', id='main')\n",
        "        if main_content_div:\n",
        "            first_p = main_content_div.find('p', recursive=True)\n",
        "\n",
        "            if first_p and first_p.get_text(strip=True):\n",
        "                raw_text = first_p.get_text(strip=True)\n",
        "\n",
        "                # Regex: Capture the first full sentence (non-greedy .*? followed by a sentence ender [.!?])\n",
        "                match = re.search(r'(.*?([.!?]))\\s*|\\Z', raw_text)\n",
        "\n",
        "                if match and match.group(1):\n",
        "                    # match.group(1) is the entire sentence up to the punctuation.\n",
        "                    snippet = match.group(1).strip()\n",
        "                else:\n",
        "                    # Fallback if no full stop is found\n",
        "                    snippet = raw_text.strip()\n",
        "\n",
        "                data['ContentSnippet'] = snippet[:250] if len(snippet) > 250 else snippet\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        data['TopicName'] = f\"ERROR: Fetch Failed\"\n",
        "        data['ContentSnippet'] = f\"Request Error: {e}\"\n",
        "\n",
        "    return data\n",
        "\n",
        "def append_to_csv(new_data: List[Dict[str, str]], filename: str):\n",
        "    \"\"\"Reads existing data, appends new data, and saves back to the file.\"\"\"\n",
        "\n",
        "    if not new_data:\n",
        "        print(\"\\n No new data scraped. File not updated.\")\n",
        "        return\n",
        "\n",
        "    df_new = pd.DataFrame(new_data)\n",
        "\n",
        "    # Read existing data or create a new DataFrame\n",
        "    if os.path.exists(filename):\n",
        "        try:\n",
        "            df_existing = pd.read_csv(filename, header=0, encoding='utf-8')\n",
        "        except Exception:\n",
        "            # If read fails, assume header is needed for the first write\n",
        "            df_existing = pd.DataFrame(columns=REQUIRED_COLUMNS)\n",
        "        print(f\" Found existing file '{filename}' with {len(df_existing)} entries.\")\n",
        "    else:\n",
        "        df_existing = pd.DataFrame(columns=REQUIRED_COLUMNS)\n",
        "        print(f\" Creating new file '{filename}'.\")\n",
        "\n",
        "    # Ensure consistent columns before concatenating\n",
        "    df_existing = df_existing.reindex(columns=REQUIRED_COLUMNS, fill_value='')\n",
        "    df_new = df_new.reindex(columns=REQUIRED_COLUMNS, fill_value='')\n",
        "\n",
        "    # Append new data\n",
        "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
        "\n",
        "    # Save the combined data back to the original file\n",
        "    df_combined.to_csv(filename, index=False, encoding='utf-8')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\" Scraping complete! Java data successfully appended to '{filename}'.\")\n",
        "    print(f\"Total entries now: {len(df_combined)}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "# MAIN EXECUTION\n",
        "if __name__ == \"__main__\":\n",
        "    # Gather new data\n",
        "    all_urls = get_all_course_links(START_URL)\n",
        "    scraped_data = []\n",
        "\n",
        "    if all_urls:\n",
        "        print(f\"\\n Starting scrape of {len(all_urls)} pages...\")\n",
        "        for i, url in enumerate(all_urls):\n",
        "            if i % 20 == 0:\n",
        "                 print(f\"   -> Progress: {i}/{len(all_urls)}\")\n",
        "            details = scrape_page_details(url)\n",
        "            scraped_data.append(details)\n",
        "            time.sleep(0.05) # Be polite\n",
        "\n",
        "    # Append to the existing CSV file\n",
        "    append_to_csv(scraped_data, CSV_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9_U_-A8KZCD",
        "outputId": "20321b05-4d7a-47ff-b6e4-8a072b371d47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Starting scrape of 3 pages...\n",
            " Scraping source 1/3\n",
            " Scraping source 2/3\n",
            " Scraping source 3/3\n",
            " Found existing file '/Users/sarahkayembe/Documents/Database_cos557/Project phase 2/data/Raw_data/course_resources.csv' with 680 entries.\n",
            "\n",
            "================================================================================\n",
            " craping complete! Criminal Law data successfully appended to '/Users/sarahkayembe/Documents/Database_cos557/Project phase 2/data/Raw_data/course_resources.csv'.\n",
            "Total entries now: 683\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Scrapes three diverse sources (Maine Legislature, APUS, edX) related to 'Criminal Law'\n",
        "# to extract the title and a brief content snippet, appending the data to the course resources CSV.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# Configuration\n",
        "\n",
        "scrapers_dir = Path(__file__).resolve().parent if \"__file__\" in locals() else Path(os.getcwd())\n",
        "project_root = scrapers_dir.parent\n",
        "\n",
        "# Build output directory path inside data/Raw_data/\n",
        "output_dir = project_root / \"data\" / \"Raw_data\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define full CSV save path\n",
        "CSV_FILENAME = \"course_resources.csv\"\n",
        "CSV_PATH = output_dir / CSV_FILENAME\n",
        "\n",
        "# Source URLs\n",
        "COURSE_NAME = \"Criminal Law\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "REQUIRED_COLUMNS = ['CourseName', 'TopicName', 'ResourceURL', 'ContentSnippet', 'ResourceType']\n",
        "\n",
        "# List of specific URLs provided by the user (excluding the Google Search link)\n",
        "CRIMINAL_LAW_URLS = [\n",
        "    \"https://legislature.maine.gov/statutes/17-a/title17-ach0sec0.html\",\n",
        "    \"https://www.apu.apus.edu/area-of-study/security-and-global-studies/resources/what-is-criminal-law-and-why-does-it-matter/\",\n",
        "    \"https://www.edx.org/learn/criminal-law\"\n",
        "]\n",
        "\n",
        "# Core Functions\n",
        "def scrape_page_details(url: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Visits a single URL and extracts the title and full first sentence for description,\n",
        "    using generic selectors for diverse sources.\n",
        "    \"\"\"\n",
        "    data = {\n",
        "        'CourseName': COURSE_NAME,\n",
        "        'TopicName': \"Title Not Found\",\n",
        "        'ResourceURL': url,\n",
        "        'ContentSnippet': \"Content Not Found\",\n",
        "        'ResourceType': 'LINK'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract Topic Name (Title)\n",
        "        title_element = soup.find(['h1', 'h2'])\n",
        "        if title_element:\n",
        "            data['TopicName'] = title_element.get_text(strip=True).strip()\n",
        "        elif soup.title:\n",
        "            # Fallback to the HTML <title> tag\n",
        "            data['TopicName'] = soup.title.get_text(strip=True).split('|')[0].strip()\n",
        "\n",
        "        # Extract Description Snippet (Full Sentences Logic)\n",
        "        # Look for the main article content (using common container IDs/classes or just the body)\n",
        "        content_containers = soup.find(['article', 'main'], class_=['content', 'entry-content', 'main-content']) or soup.body\n",
        "\n",
        "        if content_containers:\n",
        "            # Find the first few non-empty paragraphs in the content\n",
        "            first_p = content_containers.find('p', recursive=True)\n",
        "\n",
        "            if first_p and first_p.get_text(strip=True):\n",
        "                raw_text = first_p.get_text(strip=True)\n",
        "\n",
        "                # Regex: Capture the first full sentence (non-greedy .*? followed by a sentence ender [.!?])\n",
        "                match = re.search(r'(.*?([.!?]))\\s*|\\Z', raw_text)\n",
        "\n",
        "                if match and match.group(1):\n",
        "                    snippet = match.group(1).strip()\n",
        "                else:\n",
        "                    snippet = raw_text.strip()\n",
        "\n",
        "                data['ContentSnippet'] = snippet[:250] if len(snippet) > 250 else snippet\n",
        "\n",
        "            # Special handling for edX course listing page description:\n",
        "            if \"edx.org/learn/criminal-law\" in url and not data['ContentSnippet'].startswith(\"Content Not Found\"):\n",
        "                # edX uses a specific meta description which is cleaner\n",
        "                meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
        "                if meta_desc and meta_desc.get('content'):\n",
        "                    data['ContentSnippet'] = meta_desc.get('content')\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        data['TopicName'] = f\"ERROR: Fetch Failed\"\n",
        "        data['ContentSnippet'] = f\"Request Error: {e}\"\n",
        "    except Exception as e:\n",
        "        data['TopicName'] = f\"ERROR: Scrape Failed\"\n",
        "        data['ContentSnippet'] = f\"Scraping Error: {e}\"\n",
        "\n",
        "    return data\n",
        "\n",
        "def append_to_csv(new_data: List[Dict[str, str]], filename: str):\n",
        "    \"\"\"Reads existing data, appends new data, and saves back to the file.\"\"\"\n",
        "\n",
        "    if not new_data:\n",
        "        print(\"\\n No new data scraped. File not updated.\")\n",
        "        return\n",
        "\n",
        "    df_new = pd.DataFrame(new_data)\n",
        "\n",
        "    # Read existing data or create a new DataFrame\n",
        "    if os.path.exists(filename):\n",
        "        try:\n",
        "            df_existing = pd.read_csv(filename, header=0, encoding='utf-8')\n",
        "        except Exception:\n",
        "            df_existing = pd.DataFrame(columns=REQUIRED_COLUMNS)\n",
        "        print(f\" Found existing file '{filename}' with {len(df_existing)} entries.\")\n",
        "    else:\n",
        "        df_existing = pd.DataFrame(columns=REQUIRED_COLUMNS)\n",
        "        print(f\" Creating new file '{filename}'.\")\n",
        "\n",
        "    # Ensure consistent columns before concatenating\n",
        "    df_existing = df_existing.reindex(columns=REQUIRED_COLUMNS, fill_value='')\n",
        "    df_new = df_new.reindex(columns=REQUIRED_COLUMNS, fill_value='')\n",
        "\n",
        "    # Append new data\n",
        "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
        "\n",
        "    # Save the combined data back to the original file\n",
        "    df_combined.to_csv(filename, index=False, encoding='utf-8')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\" craping complete! Criminal Law data successfully appended to '{filename}'.\")\n",
        "    print(f\"Total entries now: {len(df_combined)}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "# MAIN EXECUTION\n",
        "if __name__ == \"__main__\":\n",
        "    # Gather new data from the fixed URL list\n",
        "    scraped_data = []\n",
        "\n",
        "    print(f\"\\n Starting scrape of {len(CRIMINAL_LAW_URLS)} pages...\")\n",
        "    for i, url in enumerate(CRIMINAL_LAW_URLS):\n",
        "        print(f\" Scraping source {i + 1}/{len(CRIMINAL_LAW_URLS)}\")\n",
        "        details = scrape_page_details(url)\n",
        "        scraped_data.append(details)\n",
        "        time.sleep(0.5) # Increased politeness factor for diverse sites\n",
        "\n",
        "    # Append to the existing CSV file\n",
        "    append_to_csv(scraped_data, CSV_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Mp-Pl2P-GOy",
        "outputId": "ac8cb3a3-3557-4b72-b2b8-4078fe9a787f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Starting extraction and scrape of 20 Unit Pages...\n",
            " Appended 20 unit entries to existing file '/Users/sarahkayembe/Documents/Database_cos557/Project phase 2/data/Raw_data/course_resources.csv'.\n",
            "\n",
            " Process Complete \n",
            "All unit entries, with TopicName extracted from the URL, have been processed.\n"
          ]
        }
      ],
      "source": [
        "# Scrapes 21 unit pages from Khan Academy (AP Biology and HS Biology) by extracting the course unit title\n",
        "# from the URL slug and the summary snippet from the page content, appending the results to the course resources CSV.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "\n",
        "# Configuration\n",
        "\n",
        "scrapers_dir = Path(__file__).resolve().parent if \"__file__\" in locals() else Path(os.getcwd())\n",
        "project_root = scrapers_dir.parent\n",
        "\n",
        "# Build output directory path inside data/Raw_data/\n",
        "output_dir = project_root / \"data\" / \"Raw_data\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define full CSV save path\n",
        "CSV_FILENAME = \"course_resources.csv\"\n",
        "CSV_PATH = output_dir / CSV_FILENAME\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "# The required columns for the existing CSV\n",
        "REQUIRED_COLUMNS = ['CourseName', 'TopicName', 'ResourceURL', 'ContentSnippet', 'ResourceType']\n",
        "\n",
        "# Consolidated list of 21 unique unit URLs\n",
        "FINAL_UNIT_TASKS = [\n",
        "    # Biological Principles I - 11 URLs\n",
        "    {\"CourseName\": \"Biological Principles I\", \"URL\": \"https://www.khanacademy.org/science/ap-biology/chemistry-of-life\"},\n",
        "    {\"CourseName\": \"Biological Principles I\", \"URL\": \"https://www.khanacademy.org/science/ap-biology/cell-structure-and-function\"},\n",
        "    {\"CourseName\": \"Biological Principles I\", \"URL\": \"https://www.khanacademy.org/science/ap-biology/cellular-energetics\"},\n",
        "    {\"CourseName\": \"Biological Principles I\", \"URL\": \"https://www.khanacademy.org/science/ap-biology/cell-communication-and-cell-cycle\"},\n",
        "    {\"CourseName\": \"Biological Principles I\", \"URL\": \"https://www.khanacademy.org/science/ap-biology/heredity\"},\n",
        "    {\"CourseName\": \"Biological Principles I\", \"URL\": \"https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation\"},\n",
        "    {\"CourseName\": \"Biological Principles I\", \"URL\": \"https://www.khanacademy.org/science/ap-biology/natural-selection\"},\n",
        "    {\"CourseName\": \"Biological Principles I\", \"URL\": \"https://www.khanacademy.org/science/ap-biology/ecology-ap\"},\n",
        "    {\"CourseName\": \"Biological Principles I\", \"URL\": \"https://www.khanacademy.org/science/ap-biology/x16acb03e699817e9:simulations\"},\n",
        "    {\"CourseName\": \"Biological Principles I\", \"URL\": \"https://www.khanacademy.org/science/ap-biology/worked-examples-ap-biology\"},\n",
        "    {\"CourseName\": \"Biological Principles I\", \"URL\": \"https://www.khanacademy.org/science/ap-biology/ap-biology-standards-mappings\"},\n",
        "\n",
        "    # Ecology w/ Lab - 10 URLs\n",
        "    {\"CourseName\": \"Ecology w/ Lab\", \"URL\": \"https://www.khanacademy.org/science/hs-bio/x230b3ff252126bb6:ecology-and-natural-systems\"},\n",
        "    {\"CourseName\": \"Ecology w/ Lab\", \"URL\": \"https://www.khanacademy.org/science/hs-bio/x230b3ff252126bb6:from-cells-to-organisms\"},\n",
        "    {\"CourseName\": \"Ecology w/ Lab\", \"URL\": \"https://www.khanacademy.org/science/hs-bio/x230b3ff252126bb6:the-cell-cycle-and-differentiation\"},\n",
        "    {\"CourseName\": \"Ecology w/ Lab\", \"URL\": \"https://www.khanacademy.org/science/hs-bio/x230b3ff252126bb6:energy-and-matter-in-biological-systems\"},\n",
        "    {\"CourseName\": \"Ecology w/ Lab\", \"URL\": \"https://www.khanacademy.org/science/hs-bio/x230b3ff252126bb6:gene-expression-and-regulation\"},\n",
        "    {\"CourseName\": \"Ecology w/ Lab\", \"URL\": \"https://www.khanacademy.org/science/hs-bio/x230b3ff252126bb6:inheritance-and-variation-of-traits\"},\n",
        "    {\"CourseName\": \"Ecology w/ Lab\", \"URL\": \"https://www.khanacademy.org/science/hs-bio/x230b3ff252126bb6:mechanisms-of-evolution-hs\"},\n",
        "    {\"CourseName\": \"Ecology w/ Lab\", \"URL\": \"https://www.khanacademy.org/science/hs-bio/x230b3ff252126bb6:common-ancestry-and-phylogeny\"},\n",
        "    {\"CourseName\": \"Ecology w/ Lab\", \"URL\": \"https://www.khanacademy.org/science/hs-bio/x230b3ff252126bb6:biodiversity-and-human-impacts\"},\n",
        "]\n",
        "\n",
        "def extract_topic_from_url(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts a readable topic name from a Khan Academy URL slug.\n",
        "    This logic is the primary method for unit pages.\n",
        "    \"\"\"\n",
        "    # Clean up URL: remove query parameters, anchors, and trailing slashes\n",
        "    url_cleaned = url.split('?')[0].split('#')[0].rstrip('/')\n",
        "\n",
        "    # Extract the last meaningful segment (the topic/unit slug)\n",
        "    segments = url_cleaned.split('/')\n",
        "    topic_slug = segments[-1]\n",
        "\n",
        "    # Handle the special case of the last segment being a unique identifier\n",
        "    # (like x16acb03e699817e9:simulations) - keep only the descriptive part.\n",
        "    if ':' in topic_slug:\n",
        "        topic_slug = topic_slug.split(':')[-1]\n",
        "\n",
        "    # If the slug is still a non-descriptive identifier (like x16acb03e699817e9),\n",
        "    # check the segment before it for a topic name.\n",
        "    if re.match(r'^x[0-9a-f]{16}', topic_slug, re.IGNORECASE) and len(segments) >= 2:\n",
        "         topic_slug = segments[-2]\n",
        "         if ':' in topic_slug:\n",
        "             topic_slug = topic_slug.split(':')[-1]\n",
        "\n",
        "    # Format the slug: replace hyphens/underscores with spaces and capitalize words\n",
        "    topic_name = re.sub(r'[-_]+', ' ', topic_slug)\n",
        "\n",
        "    # Capitalize the first letter of each word (Title Case)\n",
        "    return topic_name.title()\n",
        "\n",
        "\n",
        "def scrape_unit_page(task: Dict[str, Any]) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Fetches the unit page and extracts the summary, using the URL for TopicName.\n",
        "    \"\"\"\n",
        "    url = task['URL']\n",
        "    course_name = task['CourseName']\n",
        "\n",
        "    data = {\n",
        "        'CourseName': course_name,\n",
        "        'UnitURL': url,\n",
        "        'UnitTitle': extract_topic_from_url(url),\n",
        "        'UnitSummarySnippet': \"Summary Not Found\",\n",
        "    }\n",
        "\n",
        "    # Only scrape HTML for the summary\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract Unit Summary/Description\n",
        "        summary_element = None\n",
        "        # Target the main content area\n",
        "        main_content = soup.find(class_=re.compile(r'main-content|unit-description|content-container'))\n",
        "        if main_content:\n",
        "            # Find the first paragraph in the main content area\n",
        "            summary_element = main_content.find('p', recursive=True)\n",
        "\n",
        "        if summary_element and summary_element.get_text(strip=True):\n",
        "            raw_text = summary_element.get_text(strip=True)\n",
        "            # Try to get the first full sentence\n",
        "            match = re.search(r'(.*?([.!?]))\\s*|\\Z', raw_text)\n",
        "\n",
        "            snippet = match.group(1).strip() if match and match.group(1) else raw_text.strip()\n",
        "            data['UnitSummarySnippet'] = snippet[:500] if len(snippet) > 500 else snippet\n",
        "\n",
        "    except requests.exceptions.RequestException:\n",
        "        data['UnitSummarySnippet'] = \"Request Error: Failed to fetch content for summary.\"\n",
        "    except Exception:\n",
        "        data['UnitSummarySnippet'] = \"Scraping Error: Failed to parse summary.\"\n",
        "\n",
        "    return data\n",
        "\n",
        "def append_to_csv(new_data: List[Dict[str, str]], filename: str):\n",
        "    \"\"\"Appends new unit data to the existing CSV file.\"\"\"\n",
        "    if not new_data:\n",
        "        return\n",
        "\n",
        "    df_new = pd.DataFrame(new_data)\n",
        "\n",
        "    # Map and clean columns for final CSV\n",
        "    df_new = df_new.rename(columns={'UnitTitle': 'TopicName', 'UnitURL': 'ResourceURL', 'UnitSummarySnippet': 'ContentSnippet'})\n",
        "    df_new['ResourceType'] = 'UNIT'\n",
        "    df_new = df_new.reindex(columns=REQUIRED_COLUMNS, fill_value='')\n",
        "\n",
        "    try:\n",
        "        # Check if file exists and has content to decide on header\n",
        "        if os.path.exists(filename) and os.path.getsize(filename) > 0:\n",
        "            # Append without header\n",
        "            df_new.to_csv(filename, mode='a', header=False, index=False, encoding='utf-8')\n",
        "            print(f\" Appended {len(df_new)} unit entries to existing file '{filename}'.\")\n",
        "        else:\n",
        "            # Write with header (new or empty file)\n",
        "            df_new.to_csv(filename, mode='w', header=True, index=False, encoding='utf-8')\n",
        "            print(f\" Created and wrote {len(df_new)} unit entries to new file '{filename}'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL ERROR writing CSV: {e}. Data not saved.\")\n",
        "\n",
        "\n",
        "# MAIN EXECUTION\n",
        "if __name__ == \"__main__\":\n",
        "    scraped_data = []\n",
        "    processed_urls = set()\n",
        "    total_units = len(FINAL_UNIT_TASKS)\n",
        "\n",
        "    print(f\"1. Starting extraction and scrape of {total_units} Unit Pages...\")\n",
        "\n",
        "    for task in FINAL_UNIT_TASKS:\n",
        "        url = task['URL']\n",
        "        if url in processed_urls:\n",
        "            continue\n",
        "\n",
        "        details = scrape_unit_page(task)\n",
        "        scraped_data.append(details)\n",
        "        processed_urls.add(url)\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    # Append the scraped data to the existing CSV file\n",
        "    append_to_csv(scraped_data, CSV_PATH)\n",
        "\n",
        "    print(\"\\n Process Complete \")\n",
        "    print(\"All unit entries, with TopicName extracted from the URL, have been processed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output directory setted.\n",
            "Starting API fetch for 43 courses...\n",
            "\n",
            "-> Searching API for course: Analytical Chemistry with Lab with query: 'Analytical Chemistry lab techniques'\n",
            " Successfully fetched video: Top 5 Lab Techniques Every Chemistry Researcher Must Know\n",
            "\n",
            "-> Searching API for course: Organic Chemistry I with query: 'Organic Chemistry I bonding and structure tutorial'\n",
            " Successfully fetched video: Organic Chemistry Drawing Structures - Bond Line, Skeletal, and Condensed Structural Formulas\n",
            "\n",
            "-> Searching API for course: Business and Professional Communication with query: 'Professional communication skills training'\n",
            " Successfully fetched video: Give me 8 minutes, and I&#39;ll improve your communication skills by 88%...\n",
            "\n",
            "-> Searching API for course: Environmental Economics with query: 'Introduction to Environmental Economics concepts'\n",
            " Successfully fetched video: Environmental Econ: Crash Course Economics #22\n",
            "\n",
            "-> Searching API for course: Teaching Through the Arts with query: 'Integrating Arts into Classroom Teaching methods'\n",
            " Successfully fetched video: Arts Integration: Deepening Understanding of Core Content\n",
            "\n",
            "-> Searching API for course: Studies in Irish Literature and Culture with query: 'Introduction to Irish Literature and Culture summary'\n",
            " Successfully fetched video: MA Irish Literature and Culture (UCD English)\n",
            "\n",
            "-> Searching API for course: Calculus A with query: 'Calculus A limits and derivatives tutorial'\n",
            " Successfully fetched video: Calculus 1 - Introduction to Limits\n",
            "\n",
            "-> Searching API for course: Linear Algebra with query: 'Introduction to Linear Algebra and matrices lecture'\n",
            " Successfully fetched video: Intro to Matrices\n",
            "\n",
            "-> Searching API for course: Structured Problem Solving Java with query: 'Structured problem solving with Java tutorial'\n",
            " Successfully fetched video: 8 patterns to solve 80% Leetcode problems\n",
            "\n",
            "-> Searching API for course: Algorithms in Programming with query: 'Introduction to Algorithms and Data Structures'\n",
            " Successfully fetched video: Data Structures Explained for Beginners - How I Wish I was Taught\n",
            "\n",
            "-> Searching API for course: Structured Programming Laboratory with query: 'Structured programming lab exercises'\n",
            " Successfully fetched video: Think you know C programming? Test your knowledge with this MCQ!\n",
            "\n",
            "-> Searching API for course: Python Programming with query: 'Python programming complete course for beginners'\n",
            " Successfully fetched video: Python Full Course for Beginners [2025]\n",
            "\n",
            "-> Searching API for course: Programming Topics with query: 'Advanced programming topics and best practices'\n",
            " Successfully fetched video: The 3 Laws of Writing Readable Code\n",
            "\n",
            "-> Searching API for course: Computer Organization with query: 'Computer Organization architecture and design'\n",
            " Successfully fetched video: Introduction to Computer Organization and Architecture (COA)\n",
            "\n",
            "-> Searching API for course: Computer Organization Laboratory with query: 'Computer Organization lab exercises and assembly'\n",
            " Successfully fetched video: you can learn assembly in 10 minutes (try it RIGHT NOW)\n",
            "\n",
            "-> Searching API for course: Discrete Mathematics II with query: 'Discrete Mathematics II advanced topics'\n",
            " Successfully fetched video: Discrete Math Proofs in 22 Minutes (5 Types, 9 Examples)\n",
            "\n",
            "-> Searching API for course: Data Structures with query: 'Data Structures fundamental concepts and examples'\n",
            " Successfully fetched video: Data Structures and Algorithms in 15 Minutes\n",
            "\n",
            "-> Searching API for course: Systems Programming with query: 'Systems Programming concepts and C language'\n",
            " Successfully fetched video: Introduction to Programming and Computer Science - Full Course\n",
            "\n",
            "-> Searching API for course: Programming Languages with query: 'Programming Languages paradigms and concepts'\n",
            " Successfully fetched video: Ditch your Favorite Programming Paradigm\n",
            "\n",
            "-> Searching API for course: Graphical User Interface Design with query: 'GUI Design principles and tools tutorial'\n",
            " Successfully fetched video: 4 Foundational UI Design Principles | C.R.A.P.\n",
            "\n",
            "-> Searching API for course: Numerical Analysis with query: 'Numerical Analysis techniques and methods'\n",
            " Successfully fetched video: What Is Numerical Analysis?\n",
            "\n",
            "-> Searching API for course: Web Applications Development with query: 'Web Applications Development full stack tutorial'\n",
            " Successfully fetched video: How I&#39;d Learn Full-Stack Web Development (If I Could Start Over)\n",
            "\n",
            "-> Searching API for course: Programming Autonomous Robots with query: 'Programming Autonomous Robots and ROS tutorial'\n",
            " Successfully fetched video: Solving the problem EVERY robot has (with ros2_control)\n",
            "\n",
            "-> Searching API for course: Professional Ethics and Social Impact of Computing with query: 'Ethics and Social Impact of Computing lecture'\n",
            " Successfully fetched video: Definition of Ethics \n",
            "\n",
            "-> Searching API for course: Object-Oriented Design with query: 'UML and Object-Oriented Design patterns explanation'\n",
            " Successfully fetched video: 10 Design Patterns Explained in 10 Minutes\n",
            "\n",
            "-> Searching API for course: Computing for Data Science with query: 'Introduction to Computing for Data Science'\n",
            " Successfully fetched video: What is Data Science?\n",
            "\n",
            "-> Searching API for course: Mobile Development with query: 'Introduction to Mobile Development frameworks'\n",
            " Successfully fetched video: The Complete App Development Roadmap\n",
            "\n",
            "-> Searching API for course: Computational Text Analytics with query: 'Computational Text Analytics methods and tools'\n",
            " Successfully fetched video: Introduction to Computational Text Analysis with Web-Browser Tools\n",
            "\n",
            "-> Searching API for course: Software Engineering with query: 'Software Engineering principles and SDLC'\n",
            " Successfully fetched video: Introduction To Software Development LifeCycle | What Is Software Development? | Simplilearn\n",
            "\n",
            "-> Searching API for course: Deep Learning with query: 'Deep Learning Neural Networks explained'\n",
            " Successfully fetched video: Neural Networks Explained in 5 minutes\n",
            "\n",
            "-> Searching API for course: Software Project Management with query: 'Software Project Management methodologies'\n",
            " Successfully fetched video: 7 Important Project Management Methodologies You Need To MASTER\n",
            "\n",
            "-> Searching API for course: Operating Systems with query: 'Operating Systems concepts and kernel design'\n",
            " Successfully fetched video: What is a Kernel?\n",
            "\n",
            "-> Searching API for course: Computer Graphics with query: 'Computer Graphics fundamentals and rendering'\n",
            " Successfully fetched video: 3D Graphics: Crash Course Computer Science #27\n",
            "\n",
            "-> Searching API for course: Database Systems with query: 'Database Systems SQL and NoSQL tutorial'\n",
            " Successfully fetched video: SQL vs. NoSQL: What&#39;s the difference?\n",
            "\n",
            "-> Searching API for course: Computer Networks with query: 'Computer Networks protocols and TCP/IP stack'\n",
            " Successfully fetched video: What is TCP/IP?\n",
            "\n",
            "-> Searching API for course: Distributed Systems with query: 'Introduction to Distributed Systems concepts'\n",
            " Successfully fetched video: Explaining Distributed Systems Like I&#39;m 5\n",
            "\n",
            "-> Searching API for course: Compiler Construction with query: 'Compiler Construction phases and Lexical Analysis'\n",
            " Successfully fetched video: Different Phases of Compiler\n",
            "\n",
            "-> Searching API for course: Topics in Computer Science with query: 'Advanced Topics in Computer Science lectures'\n",
            " Successfully fetched video: 100+ Computer Science Concepts Explained\n",
            "\n",
            "-> Searching API for course: Artificial Intelligence and Data Mining with query: 'AI and Data Mining concepts and applications'\n",
            " Successfully fetched video: Data Mining for Beginners: Concepts, Techniques &amp; Applications\n",
            "\n",
            "-> Searching API for course: Machine Learning with query: 'Machine Learning fundamental concepts and algorithms'\n",
            " Successfully fetched video: All Machine Learning algorithms explained in 17 min\n",
            "\n",
            "-> Searching API for course: Design and Analysis of Computing Algorithms with query: 'Design and Analysis of Algorithms detailed course'\n",
            " Successfully fetched video: Algorithms and Data Structures Tutorial - Full Course for Beginners\n",
            "\n",
            "-> Searching API for course: Independent Study in Computer Science with query: 'Computer Science independent research ideas'\n",
            " Successfully fetched video: My Top Tips For Computer Science Students\n",
            "\n",
            "-> Searching API for course: Computer Science Internship with query: 'Computer Science Internship preparation and advice'\n",
            " Successfully fetched video: How to NOT Fail a Technical Interview\n",
            "\n",
            "Finished fetching. Appending 43 total entries to CSV.\n",
            " Found existing file 'course_resources.csv' with 703 entries.\n",
            "\n",
            "======================================================================\n",
            " Data successfully appended.\n",
            "Total entries now: 746\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Automated system to fetch and log external learning resources (YouTube videos)\n",
        "# for a list of courses using the YouTube API, storing results in a CSV.\n",
        "\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Union\n",
        "from pathlib import Path\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Configuration\n",
        "API_KEY = '' # REPLACE THIS WITH YOUR ACTUAL API KEY\n",
        "YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
        "YOUTUBE_API_VERSION = 'v3'\n",
        "\n",
        "# Detect current script location\n",
        "current_path = Path.cwd() \n",
        "\n",
        "# Go up to the project root\n",
        "project_root = current_path.parent\n",
        "\n",
        "# Constructing the full output path: project_root / 'data' / 'Raw_data'\n",
        "output_dir = project_root / 'data' / 'Raw_data'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Output directory setted.\")\n",
        "\n",
        "# Defining full save path\n",
        "CSV_FILENAME = \"course_resources.csv\"\n",
        "CSV_PATH = output_dir / CSV_FILENAME \n",
        "\n",
        "# NEW: Comprehensive List of Courses and Search Queries\n",
        "COURSE_LIST = {\n",
        "    # Non-CS Courses from previous list\n",
        "    'Analytical Chemistry with Lab': 'Analytical Chemistry lab techniques',\n",
        "    'Organic Chemistry I': 'Organic Chemistry I bonding and structure tutorial',\n",
        "    'Business and Professional Communication': 'Professional communication skills training',\n",
        "    'Environmental Economics': 'Introduction to Environmental Economics concepts',\n",
        "    'Teaching Through the Arts': 'Integrating Arts into Classroom Teaching methods',\n",
        "    'Studies in Irish Literature and Culture': 'Introduction to Irish Literature and Culture summary',\n",
        "    'Calculus A': 'Calculus A limits and derivatives tutorial',\n",
        "    'Linear Algebra': 'Introduction to Linear Algebra and matrices lecture',\n",
        "    \n",
        "    # New Computer Science Courses\n",
        "    'Structured Problem Solving Java': 'Structured problem solving with Java tutorial',\n",
        "    'Algorithms in Programming': 'Introduction to Algorithms and Data Structures',\n",
        "    'Structured Programming Laboratory': 'Structured programming lab exercises',\n",
        "    'Python Programming': 'Python programming complete course for beginners',\n",
        "    'Programming Topics': 'Advanced programming topics and best practices',\n",
        "    'Computer Organization': 'Computer Organization architecture and design',\n",
        "    'Computer Organization Laboratory': 'Computer Organization lab exercises and assembly',\n",
        "    'Discrete Mathematics II': 'Discrete Mathematics II advanced topics',\n",
        "    'Data Structures': 'Data Structures fundamental concepts and examples',\n",
        "    'Systems Programming': 'Systems Programming concepts and C language',\n",
        "    'Programming Languages': 'Programming Languages paradigms and concepts',\n",
        "    'Graphical User Interface Design': 'GUI Design principles and tools tutorial',\n",
        "    'Numerical Analysis': 'Numerical Analysis techniques and methods',\n",
        "    'Web Applications Development': 'Web Applications Development full stack tutorial',\n",
        "    'Programming Autonomous Robots': 'Programming Autonomous Robots and ROS tutorial',\n",
        "    'Professional Ethics and Social Impact of Computing': 'Ethics and Social Impact of Computing lecture',\n",
        "    'Object-Oriented Design': 'UML and Object-Oriented Design patterns explanation',\n",
        "    'Computing for Data Science': 'Introduction to Computing for Data Science',\n",
        "    'Mobile Development': 'Introduction to Mobile Development frameworks',\n",
        "    'Computational Text Analytics': 'Computational Text Analytics methods and tools',\n",
        "    'Software Engineering': 'Software Engineering principles and SDLC',\n",
        "    'Deep Learning': 'Deep Learning Neural Networks explained',\n",
        "    'Software Project Management': 'Software Project Management methodologies',\n",
        "    'Operating Systems': 'Operating Systems concepts and kernel design',\n",
        "    'Computer Graphics': 'Computer Graphics fundamentals and rendering',\n",
        "    'Database Systems': 'Database Systems SQL and NoSQL tutorial',\n",
        "    'Computer Networks': 'Computer Networks protocols and TCP/IP stack',\n",
        "    'Distributed Systems': 'Introduction to Distributed Systems concepts',\n",
        "    'Compiler Construction': 'Compiler Construction phases and Lexical Analysis',\n",
        "    'Topics in Computer Science': 'Advanced Topics in Computer Science lectures',\n",
        "    'Artificial Intelligence and Data Mining': 'AI and Data Mining concepts and applications',\n",
        "    'Machine Learning': 'Machine Learning fundamental concepts and algorithms',\n",
        "    'Design and Analysis of Computing Algorithms': 'Design and Analysis of Algorithms detailed course',\n",
        "    'Independent Study in Computer Science': 'Computer Science independent research ideas',\n",
        "    'Computer Science Internship': 'Computer Science Internship preparation and advice',\n",
        "}\n",
        "\n",
        "# API FUNCTION\n",
        "def fetch_youtube_video(course_name: str, query: str) -> Union[Dict[str, str], None]:\n",
        "    \"\"\"\n",
        "    Uses the YouTube Data API to search for the most relevant video\n",
        "    and formats the result as a resource entry.\n",
        "    \"\"\"\n",
        "    print(f\"\\n-> Searching API for course: {course_name} with query: '{query}'\")\n",
        "    \n",
        "    try:\n",
        "        # Initialize the API client\n",
        "        youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)\n",
        "        \n",
        "        # Execute the search.list request\n",
        "        search_response = youtube.search().list(\n",
        "            q=query,\n",
        "            part='snippet',\n",
        "            maxResults=1,  # Get only the top result\n",
        "            type='video'\n",
        "        ).execute()\n",
        "\n",
        "        # Check if a video was found\n",
        "        if not search_response.get('items'):\n",
        "            print(f\" No results found for {course_name}.\")\n",
        "            return None\n",
        "\n",
        "        item = search_response['items'][0]\n",
        "        video_id = item['id']['videoId']\n",
        "        \n",
        "        # Extract and format the data\n",
        "        topic_data = {\n",
        "            'CourseName': course_name,\n",
        "            'TopicName': item['snippet']['title'],\n",
        "            'ContentSnippet': item['snippet']['description'][:250] + \"...\",\n",
        "            'ResourceURL': f'https://www.youtube.com/watch?v={video_id}',\n",
        "            'ResourceType': 'YOUTUBE_VIDEO'\n",
        "        }\n",
        "        print(f\" Successfully fetched video: {topic_data['TopicName']}\")\n",
        "        return topic_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" ERROR using YouTube API for {course_name}: {e}\")\n",
        "        print(\" Please check your API key, quotas, and network connection.\")\n",
        "        return None\n",
        "\n",
        "# --- APPEND FUNCTION ---\n",
        "def append_to_csv(new_data: List[Dict[str, str]], filepath: Path):\n",
        "    \"\"\"\n",
        "    Reads existing data, appends new data, and saves back to the file using\n",
        "    the Path object for reliable path handling.\n",
        "    \"\"\"\n",
        "\n",
        "    df_new = pd.DataFrame(new_data)\n",
        "\n",
        "    # Read existing data, or create an empty DataFrame if file doesn't exist\n",
        "    if filepath.exists(): \n",
        "        df_existing = pd.read_csv(filepath)\n",
        "        print(f\" Found existing file '{filepath.name}' with {len(df_existing)} entries.\")\n",
        "    else:\n",
        "        # Create a new DataFrame with the correct column structure if the file doesn't exist\n",
        "        df_existing = pd.DataFrame(columns=df_new.columns) \n",
        "        print(f\" Creating new file '{filepath.name}'.\")\n",
        "\n",
        "    # Ensure consistent columns before concatenating\n",
        "    df_existing = df_existing.reindex(columns=df_new.columns, fill_value='')\n",
        "\n",
        "    # Append new data\n",
        "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
        "\n",
        "    # Save the combined data back to the original file\n",
        "    df_combined.to_csv(filepath, index=False, encoding='utf-8')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\" Data successfully appended.\")\n",
        "    print(f\"Total entries now: {len(df_combined)}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "# MAIN EXECUTION (LOOPING)\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # Initialize a list to hold all entries from all courses\n",
        "    all_new_entries = []\n",
        "    print(f\"Starting API fetch for {len(COURSE_LIST)} courses...\")\n",
        "\n",
        "    # Loop through the list of courses\n",
        "    for course_name, search_query in COURSE_LIST.items():\n",
        "        \n",
        "        # Fetch YouTube video data via API for the current course\n",
        "        youtube_entry = fetch_youtube_video(course_name, search_query)\n",
        "        \n",
        "        if youtube_entry:\n",
        "            all_new_entries.append(youtube_entry)\n",
        "\n",
        "    # Append all collected entries to the CSV file\n",
        "    if all_new_entries:\n",
        "        print(f\"\\nFinished fetching. Appending {len(all_new_entries)} total entries to CSV.\")\n",
        "        append_to_csv(all_new_entries, CSV_PATH)\n",
        "    else:\n",
        "        print(\"\\n No new entries were successfully generated to append to the CSV. Check API key and quota.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
