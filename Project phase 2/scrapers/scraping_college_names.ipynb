{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe963be4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe963be4",
        "outputId": "628a4dba-b4d4-431a-a81f-1cde23e6aaab"
      },
      "outputs": [],
      "source": [
        "# COLLECTING COLLEGES NAMES AND STATES\n",
        "\"\"\"\n",
        "U.S. College Name Scraper\n",
        "-------------------------\n",
        "This script scrapes lists of American colleges and universities from Wikipedia.\n",
        "It starts from the main \"Lists of American universities and colleges\" hub page,\n",
        "follows each state-specific link, and extracts institution names from tables.\n",
        "\n",
        "Key Features:\n",
        "- Dynamically fetches and parses all U.S. state pages from Wikipedia.\n",
        "- Cleans data to remove citations and duplicates.\n",
        "- Outputs two CSV files:\n",
        "    - ../data/us_colleges.csv          → Raw scraped data\n",
        "    - ../data/clean/us_colleges_clean.csv → Cleaned, deduplicated data\n",
        "- Includes retry logic and polite delays to avoid IP throttling.\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "from io import StringIO\n",
        "import warnings\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# suppress pandas warning about SSL/certificate checks\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# configurations\n",
        "BASE_URL = \"https://en.wikipedia.org\"\n",
        "HUB_PAGE_URL = \"https://en.wikipedia.org/wiki/Lists_of_American_universities_and_colleges\"\n",
        "\n",
        "headers = {\n",
        "    # using a standard User-Agent to bypass the 403 error\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# function to get all state links\n",
        "def get_all_state_links():\n",
        "    \"\"\"Fetches the hub page and extracts all state list URLs.\"\"\"\n",
        "    state_urls = {}\n",
        "    print(f\"Fetching link hub: {HUB_PAGE_URL}\")\n",
        "\n",
        "    # \n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = requests.get(HUB_PAGE_URL, headers=headers, timeout=15)\n",
        "            if response.status_code == 200:\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Hub attempt {attempt+1}: status {response.status_code}\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Hub attempt {attempt+1} failed: {e}\")\n",
        "            time.sleep(2 + random.random())\n",
        "    else:\n",
        "        print(\"Failed to fetch hub page after 3 attempts.\")\n",
        "        return {}\n",
        "    #\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    body_content = soup.find(id=\"bodyContent\")\n",
        "\n",
        "    if body_content:\n",
        "        state_links = body_content.find_all(\n",
        "            'a',\n",
        "            href=lambda href: href and href.startswith('/wiki/List_of_colleges_and_universities_in_')\n",
        "        )\n",
        "        for link in state_links:\n",
        "            state_name = link.get_text(strip=True)\n",
        "            absolute_url = urljoin(BASE_URL, link['href'])\n",
        "            state_urls[state_name] = absolute_url\n",
        "\n",
        "    return state_urls\n",
        "\n",
        "\n",
        "# function to Scrape Colleges from a Single State Page (CLEANED)\n",
        "def scrape_colleges_from_state_page(url, state_name):\n",
        "    \"\"\"Finds the wikitable, reads it with pandas, and extracts the college names.\"\"\"\n",
        "    colleges = []\n",
        "\n",
        "    # using 'School' as the primary match\n",
        "    possible_college_columns = ['School', 'Institution', 'University', 'College', 'Name', 0]\n",
        "\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=15)\n",
        "            if response.status_code == 200:\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Attempt {attempt+1}: {state_name} returned status {response.status_code}\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Attempt {attempt+1}: Error fetching {state_name} ({e})\")\n",
        "            time.sleep(2 + random.random())  # wait a bit before retry\n",
        "    else:\n",
        "        print(f\"Skipping {state_name} after 3 failed attempts.\")\n",
        "        return colleges\n",
        "    \n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # using BeautifulSoup to find all tables with the specific Wikipedia class\n",
        "    wiki_tables = soup.find_all('table', {'class': 'wikitable'})\n",
        "\n",
        "    if not wiki_tables:\n",
        "        return colleges\n",
        "\n",
        "    # prioritizing the table with the most rows\n",
        "    best_table = None\n",
        "    best_size = 0\n",
        "    for table in wiki_tables:\n",
        "        row_count = len(table.find_all('tr'))\n",
        "        if row_count > best_size:\n",
        "            best_size = row_count\n",
        "            best_table = table\n",
        "\n",
        "    if not best_table:\n",
        "        return colleges\n",
        "\n",
        "    table_html = str(best_table)\n",
        "\n",
        "    # using pandas.read_html on the SINGLE table's HTML string\n",
        "    data_frames = pd.read_html(StringIO(table_html), header=0)\n",
        "\n",
        "    if not data_frames:\n",
        "            return colleges\n",
        "\n",
        "    df = data_frames[0]\n",
        "\n",
        "    # search the DataFrame's columns for the college name\n",
        "    for col_name in possible_college_columns:\n",
        "        if col_name in df.columns:\n",
        "            names = df[col_name].dropna().astype(str).tolist()\n",
        "            # cleaning by removing bracketed citations like [1], [a], etc.\n",
        "            names = [name.split('[')[0].strip() for name in names]\n",
        "            colleges.extend(names)\n",
        "            time.sleep(1 + random.random()) \n",
        "            return colleges # Success! Exit the function\n",
        "\n",
        "    return colleges\n",
        "\n",
        "# final main execution loop (FULL CRAWL)\n",
        "def run_scraper():\n",
        "    \"\"\"Executes the entire scraping process across all states and saves to CSV.\"\"\"\n",
        "    all_college_data = []\n",
        "\n",
        "    # get all state links\n",
        "    state_links = get_all_state_links()\n",
        "    if not state_links:\n",
        "        print(\"Scraper aborted: Could not get state links.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(state_links)} state/territory links.\")\n",
        "    print(f\"\\nStarting full scraping process for all {len(state_links)} pages...\")\n",
        "\n",
        "    # iterating through each state and scraping its page\n",
        "    for i, (state, url) in enumerate(state_links.items()):\n",
        "        print(f\"  [{i+1}/{len(state_links)}] Scraping colleges in {state}...\")\n",
        "\n",
        "        colleges = scrape_colleges_from_state_page(url, state)\n",
        "\n",
        "        if colleges:\n",
        "            all_college_data.append({\n",
        "                'State': state,\n",
        "                'Colleges': colleges,\n",
        "                'Count': len(colleges)\n",
        "            })\n",
        "            print(f\"    -> Extracted {len(colleges)} institutions.\")\n",
        "        else:\n",
        "            print(f\"    -> No college data extracted for {state}. (Structure may be non-standard)\")\n",
        "\n",
        "    # final Aggregation and Output\n",
        "    total_institutions = sum(item['Count'] for item in all_college_data)\n",
        "\n",
        "    print(\"\\n==============================================\")\n",
        "    print(\"SCRAPING COMPLETE\")\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Total states/territories successfully scraped: {len(all_college_data)} out of {len(state_links)}\")\n",
        "    print(f\"Total institutions collected: {total_institutions:,}\")\n",
        "    print(\"----------------------------------------------\")\n",
        "\n",
        "    # converting to a flat DataFrame and saving the data\n",
        "    all_colleges_flat = []\n",
        "    for item in all_college_data:\n",
        "        for college_name in item['Colleges']:\n",
        "            all_colleges_flat.append({'State': item['State'], 'College Name': college_name})\n",
        "\n",
        "    df_final = pd.DataFrame(all_colleges_flat)\n",
        "\n",
        "    output_filename = \"us_colleges.csv\"\n",
        "    current_dir = Path(os.getcwd())                          \n",
        "    # If currently inside 'scrapers', move up one level to project root\n",
        "    if current_dir.name == \"scrapers\":\n",
        "        project_root = current_dir.parent\n",
        "    else:\n",
        "        project_root = current_dir\n",
        "\n",
        "    # Define path to data/Raw_data\n",
        "    data_dir = project_root / \"data\" / \"Raw_data\"\n",
        "    data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Final file path\n",
        "    output_path = data_dir / output_filename\n",
        "\n",
        "    print(f\"\\nSaving data to: {output_path}\")\n",
        "    df_final.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "    print(\"Finished. Data saved successfully.\")\n",
        "\n",
        "# executing the final function\n",
        "if __name__ == \"__main__\":\n",
        "    run_scraper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "023b7d44",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "--TdJRvdWpg5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--TdJRvdWpg5",
        "outputId": "0a2aae0c-03f4-4b91-a1eb-780f371976b5"
      },
      "outputs": [],
      "source": [
        "# Cleaning the U.S. Colleges CSV\n",
        "\"\"\"\n",
        "This script loads the raw 'us_colleges.csv' from data/Raw_data/,\n",
        "cleans and deduplicates it, and saves the final file into data/clean/.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Path setup (works in both scrapers/ and project root)\n",
        "\n",
        "current_dir = Path(os.getcwd())\n",
        "\n",
        "# If inside 'scrapers', go up to project root\n",
        "if current_dir.name == \"scrapers\":\n",
        "    project_root = current_dir.parent\n",
        "else:\n",
        "    project_root = current_dir\n",
        "\n",
        "# Define input and output folders\n",
        "raw_data_dir = project_root / \"data\" / \"Raw_data\"\n",
        "clean_data_dir = project_root / \"data\" / \"clean\"\n",
        "clean_data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# File paths\n",
        "input_path = raw_data_dir / \"us_colleges.csv\"\n",
        "output_path = clean_data_dir / \"us_colleges_clean.csv\"\n",
        "\n",
        "# Load the CSV file\n",
        "print(f\"Loading file from: {input_path}\")\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Normalize column names and values\n",
        "df['College Name'] = df['College Name'].astype(str).str.strip()\n",
        "df['State'] = df['State'].astype(str).str.strip()\n",
        "\n",
        "# Detect and show exact duplicates\n",
        "duplicates_mask = df.duplicated(subset=['College Name', 'State'], keep='first')\n",
        "duplicates_df = df[duplicates_mask]\n",
        "\n",
        "if not duplicates_df.empty:\n",
        "    print(\"\\nExact duplicates to be removed:\")\n",
        "    for _, row in duplicates_df.iterrows():\n",
        "        print(f\" - {row['College Name']} ({row['State']})\")\n",
        "else:\n",
        "    print(\"\\nNo exact duplicates found to remove.\")\n",
        "\n",
        "# Drop duplicates and sort\n",
        "df_clean = df.drop_duplicates(subset=['College Name', 'State'], keep='first')\n",
        "df_clean = df_clean.sort_values(by=['State', 'College Name']).reset_index(drop=True)\n",
        "\n",
        "# Save cleaned data\n",
        "df_clean.to_csv(output_path, index=False, encoding='utf-8')\n",
        "\n",
        "# Summary\n",
        "print(\"\\n--------------------------------\")\n",
        "print(f\"Before: {len(df)} rows\")\n",
        "print(f\"After:  {len(df_clean)} rows\")\n",
        "print(f\"Removed: {len(df) - len(df_clean)} exact duplicates\")\n",
        "print(f\"Clean file saved to: {output_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
