{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbuAvGBJ7-xz",
        "outputId": "e988279a-b4ae-46cb-deaf-7d51e258d94f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detecting total number of pages from a content page (Page 2)...\n",
            "could not find exact page count. Assuming 18 pages for safe measure.\n",
            "Total pages detected: 18\n",
            "Scraping course page 2/18...\n",
            "Scraping course page 3/18...\n",
            "Scraping course page 4/18...\n",
            "Scraping course page 5/18...\n",
            "Scraping course page 6/18...\n",
            "Scraping course page 7/18...\n",
            "Scraping course page 8/18...\n",
            "Scraping course page 9/18...\n",
            "Scraping course page 10/18...\n",
            "Scraping course page 11/18...\n",
            "Scraping course page 12/18...\n",
            "Scraping course page 13/18...\n",
            "Scraping course page 14/18...\n",
            "Scraping course page 15/18...\n",
            "Scraping course page 16/18...\n",
            "Scraping course page 17/18...\n",
            "Scraping course page 18/18...\n",
            "\n",
            " Scraped 1699 total courses across 18 pages.\n",
            "Saved 1699 unique courses.\n"
          ]
        }
      ],
      "source": [
        "# COLLECTING COURSE NAMES AND CODE\n",
        "\"\"\"\n",
        "USM Course Scraper\n",
        "------------------\n",
        "This script scrapes course data from the University of Southern Maine's online catalog.\n",
        "It navigates through multiple catalog pages, extracts course names, codes, and related\n",
        "details, and stores them in a structured CSV file located in the 'data' folder.\n",
        "\n",
        "Key Features:\n",
        "- Fetches and parses course listings from multiple pages.\n",
        "- Deduplicates entries based on course name and code.\n",
        "- Saves results to: ../data/usm_courses.csv\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "import os\n",
        "\n",
        "# Base URL pattern\n",
        "base_url = \"https://catalog.usm.maine.edu/content.php\"\n",
        "params_base = {\n",
        "    \"catoid\": 3,\n",
        "    \"navoid\": 80,\n",
        "    \"filter[item_type]\": 3,\n",
        "    \"filter[only_active]\": 1,\n",
        "    \"filter[3]\": 1,\n",
        "    # filter[cpage] will be changed dynamically\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
        "    ),\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "}\n",
        "\n",
        "# Correctly Detect total number of pages\n",
        "# Probing a page that contains content (e.g., Page 2) to find the pagination control.\n",
        "print(\"Detecting total number of pages from a content page (Page 2)...\")\n",
        "\n",
        "test_params = params_base.copy()\n",
        "# Probing Page 2, as Page 1 is an empty search form\n",
        "test_params[\"filter[cpage]\"] = 2\n",
        "\n",
        "try:\n",
        "    response = requests.get(base_url, headers=headers, params=test_params)\n",
        "    response.raise_for_status()\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Looking for the pagination text, e.g., \"Page 2 of 18\"\n",
        "    # Looking for the text node containing the pattern\n",
        "    page_info = soup.find(string=re.compile(r\"Page\\s+\\d+\\s+of\\s+\\d+\"))\n",
        "\n",
        "    if page_info:\n",
        "        # Extracting the total number (the second digit)\n",
        "        match = re.search(r\"Page\\s+\\d+\\s+of\\s+(\\d+)\", page_info)\n",
        "        total_pages = int(match.group(1)) if match else 1\n",
        "    else:\n",
        "        # Fallback to a safe number if detection fails, assuming at least a few pages exist\n",
        "        total_pages = 18\n",
        "        print(f\"could not find exact page count. Assuming {total_pages} pages for safe measure.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"error during page detection: {e}. Aborting scrape.\")\n",
        "    total_pages = 0\n",
        "\n",
        "\n",
        "print(f\"Total pages detected: {total_pages}\")\n",
        "\n",
        "# Looping through each page, starting from 2\n",
        "all_courses: List[Dict[str, Any]] = []\n",
        "\n",
        "start_page = 2 if total_pages >= 2 else 1\n",
        "for page in range(start_page, total_pages + 1):\n",
        "    print(f\"Scraping course page {page}/{total_pages}...\")\n",
        "\n",
        "    params = params_base.copy()\n",
        "    params[\"filter[cpage]\"] = page\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, headers=headers, params=params)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"HTTP Error scraping page {page}: {e}. Stopping pagination.\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Finding all course links using 'preview_course_nopop.php' URL fragment\n",
        "    course_links = soup.find_all(\"a\", href=lambda href: href and \"preview_course_nopop.php\" in href)\n",
        "\n",
        "    if not course_links:\n",
        "        print(f\"No courses found on page {page}. Assuming end of course listings.\")\n",
        "        # If a page in the middle is empty, we assume the catalog has ended\n",
        "        break\n",
        "\n",
        "    # Extracting course info\n",
        "    for a in course_links:\n",
        "        text = a.get_text(strip=True)\n",
        "        # Constructing a full URL using urljoin\n",
        "        href = urljoin(base_url, a.get(\"href\"))\n",
        "\n",
        "        if \" - \" in text:\n",
        "            code, name = text.split(\" - \", 1)\n",
        "        else:\n",
        "            # Handle cases where \" - \" might be missing (use the full text as the name)\n",
        "            code, name = \"N/A\", text\n",
        "\n",
        "        all_courses.append({\n",
        "            \"course_code\": code.strip(),\n",
        "            \"course_name\": name.strip(),\n",
        "            \"course_link\": href,\n",
        "            \"page_number\": page\n",
        "        })\n",
        "\n",
        "    time.sleep(0.5)  # Be polite to the server :)\n",
        "\n",
        "# Saving to CSV\n",
        "if all_courses:\n",
        "    print(f\"\\n Scraped {len(all_courses)} total courses across {total_pages} pages.\")\n",
        "\n",
        "    df = pd.DataFrame(all_courses)\n",
        "    # Deduplicating based on unique code/name pairs\n",
        "    df.drop_duplicates(subset=[\"course_code\", \"course_name\"], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    output_filename = \"usm_courses.csv\"\n",
        "    current_dir = os.getcwd()\n",
        "    data_dir = os.path.join(current_dir, \"..\", \"data\")\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    output_path = os.path.join(data_dir, output_filename)\n",
        "\n",
        "    # Save CSV in the data folder\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Saved {len(df)} unique courses.\")\n",
        "else:\n",
        "    print(\"\\n FAILURE: No course data was successfully scraped.\")\n",
        "\n",
        "# Display the first 20 records for quick verification (Testing)\n",
        "# display(df.head(20))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
