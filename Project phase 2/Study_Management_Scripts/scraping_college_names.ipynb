{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fe963be4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe963be4",
        "outputId": "628a4dba-b4d4-431a-a81f-1cde23e6aaab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching link hub: https://en.wikipedia.org/wiki/Lists_of_American_universities_and_colleges\n",
            "Found 58 state/territory links.\n",
            "\n",
            "Starting full scraping process for all 58 pages...\n",
            "  [1/58] Scraping colleges in Connecticut...\n",
            "    -> Extracted 36 institutions.\n",
            "  [2/58] Scraping colleges in Maine...\n",
            "    -> Extracted 32 institutions.\n",
            "  [3/58] Scraping colleges in Massachusetts...\n",
            "    -> Extracted 101 institutions.\n",
            "  [4/58] Scraping colleges in New Hampshire...\n",
            "    -> No college data extracted for New Hampshire. (Structure may be non-standard)\n",
            "  [5/58] Scraping colleges in New Jersey...\n",
            "    -> Extracted 34 institutions.\n",
            "  [6/58] Scraping colleges in New York...\n",
            "    -> No college data extracted for New York. (Structure may be non-standard)\n",
            "  [7/58] Scraping colleges in Pennsylvania...\n",
            "    -> Extracted 280 institutions.\n",
            "  [8/58] Scraping colleges in Rhode Island...\n",
            "    -> Extracted 13 institutions.\n",
            "  [9/58] Scraping colleges in Vermont...\n",
            "    -> Extracted 19 institutions.\n",
            "  [10/58] Scraping colleges in Illinois...\n",
            "    -> Extracted 58 institutions.\n",
            "  [11/58] Scraping colleges in Indiana...\n",
            "    -> Extracted 67 institutions.\n",
            "  [12/58] Scraping colleges in Iowa...\n",
            "    -> Extracted 52 institutions.\n",
            "  [13/58] Scraping colleges in Kansas...\n",
            "    -> No college data extracted for Kansas. (Structure may be non-standard)\n",
            "  [14/58] Scraping colleges in Michigan...\n",
            "    -> No college data extracted for Michigan. (Structure may be non-standard)\n",
            "  [15/58] Scraping colleges in Minnesota...\n",
            "    -> Extracted 74 institutions.\n",
            "  [16/58] Scraping colleges in Missouri...\n",
            "    -> Extracted 23 institutions.\n",
            "  [17/58] Scraping colleges in Nebraska...\n",
            "    -> Extracted 19 institutions.\n",
            "  [18/58] Scraping colleges in North Dakota...\n",
            "    -> Extracted 20 institutions.\n",
            "  [19/58] Scraping colleges in Ohio...\n",
            "    -> Extracted 103 institutions.\n",
            "  [20/58] Scraping colleges in South Dakota...\n",
            "    -> No college data extracted for South Dakota. (Structure may be non-standard)\n",
            "  [21/58] Scraping colleges in Wisconsin...\n",
            "    -> No college data extracted for Wisconsin. (Structure may be non-standard)\n",
            "  [22/58] Scraping colleges in Alabama...\n",
            "    -> Extracted 60 institutions.\n",
            "  [23/58] Scraping colleges in Arkansas...\n",
            "    -> Extracted 48 institutions.\n",
            "  [24/58] Scraping colleges in Delaware...\n",
            "    -> Extracted 6 institutions.\n",
            "  [25/58] Scraping colleges in Florida...\n",
            "    -> No college data extracted for Florida. (Structure may be non-standard)\n",
            "  [26/58] Scraping colleges in Georgia...\n",
            "    -> Extracted 52 institutions.\n",
            "  [27/58] Scraping colleges in Kentucky...\n",
            "    -> Extracted 9 institutions.\n",
            "  [28/58] Scraping colleges in Louisiana...\n",
            "    -> No college data extracted for Louisiana. (Structure may be non-standard)\n",
            "  [29/58] Scraping colleges in Maryland...\n",
            "    -> Extracted 24 institutions.\n",
            "  [30/58] Scraping colleges in Mississippi...\n",
            "    -> Extracted 33 institutions.\n",
            "  [31/58] Scraping colleges in North Carolina...\n",
            "    -> Extracted 64 institutions.\n",
            "  [32/58] Scraping colleges in Oklahoma...\n",
            "    -> Extracted 33 institutions.\n",
            "  [33/58] Scraping colleges in South Carolina...\n",
            "    -> Extracted 36 institutions.\n",
            "  [34/58] Scraping colleges in Tennessee...\n",
            "    -> Extracted 55 institutions.\n",
            "  [35/58] Scraping colleges in Texas...\n",
            "    -> Extracted 56 institutions.\n",
            "  [36/58] Scraping colleges in Virginia...\n",
            "    -> Extracted 81 institutions.\n",
            "  [37/58] Scraping colleges in West Virginia...\n",
            "    -> Extracted 38 institutions.\n",
            "  [38/58] Scraping colleges in Alaska...\n",
            "    -> Extracted 6 institutions.\n",
            "  [39/58] Scraping colleges in Arizona...\n",
            "    -> Extracted 27 institutions.\n",
            "  [40/58] Scraping colleges in California...\n",
            "    -> Extracted 141 institutions.\n",
            "  [41/58] Scraping colleges in Colorado...\n",
            "    -> Extracted 23 institutions.\n",
            "  [42/58] Scraping colleges in Hawaii...\n",
            "    -> Extracted 16 institutions.\n",
            "  [43/58] Scraping colleges in Idaho...\n",
            "    -> Extracted 15 institutions.\n",
            "  [44/58] Scraping colleges in Montana...\n",
            "    -> Extracted 9 institutions.\n",
            "  [45/58] Scraping colleges in Nevada...\n",
            "    -> Extracted 8 institutions.\n",
            "  [46/58] Scraping colleges in New Mexico...\n",
            "    -> Extracted 31 institutions.\n",
            "  [47/58] Scraping colleges in Oregon...\n",
            "    -> Extracted 50 institutions.\n",
            "  [48/58] Scraping colleges in Utah...\n",
            "    -> Extracted 19 institutions.\n",
            "  [49/58] Scraping colleges in Washington...\n",
            "    -> Extracted 13 institutions.\n",
            "  [50/58] Scraping colleges in Wyoming...\n",
            "    -> Extracted 12 institutions.\n",
            "  [51/58] Scraping colleges in Washington, D.C....\n",
            "    -> Extracted 18 institutions.\n",
            "  [52/58] Scraping colleges in Guam...\n",
            "    -> No college data extracted for Guam. (Structure may be non-standard)\n",
            "  [53/58] Scraping colleges in Puerto Rico...\n",
            "    -> No college data extracted for Puerto Rico. (Structure may be non-standard)\n",
            "  [54/58] Scraping colleges in U.S. Virgin Islands...\n",
            "    -> No college data extracted for U.S. Virgin Islands. (Structure may be non-standard)\n",
            "  [55/58] Scraping colleges in List of colleges and universities in the United States by endowment...\n",
            "    -> Extracted 88 institutions.\n",
            "  [56/58] Scraping colleges in American Samoa...\n",
            "    -> No college data extracted for American Samoa. (Structure may be non-standard)\n",
            "  [57/58] Scraping colleges in Northern Mariana Islands...\n",
            "    -> No college data extracted for Northern Mariana Islands. (Structure may be non-standard)\n",
            "  [58/58] Scraping colleges in Endowment...\n",
            "    -> Extracted 88 institutions.\n",
            "\n",
            "==============================================\n",
            "SCRAPING COMPLETE\n",
            "==============================================\n",
            "Total states/territories successfully scraped: 45 out of 58\n",
            "Total institutions collected: 2,090\n",
            "----------------------------------------------\n",
            "\n",
            "Saving data to 'us_colleges.csv'...\n",
            "Finished. Data saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# COLLECTING COLLEGES NAMES AND STATES\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "from io import StringIO\n",
        "import warnings\n",
        "\n",
        "# suppress pandas warning about SSL/certificate checks\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# configurations\n",
        "BASE_URL = \"https://en.wikipedia.org\"\n",
        "HUB_PAGE_URL = \"https://en.wikipedia.org/wiki/Lists_of_American_universities_and_colleges\"\n",
        "\n",
        "headers = {\n",
        "    # using a standard User-Agent to bypass the 403 error\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# function to get all state links\n",
        "def get_all_state_links():\n",
        "    \"\"\"Fetches the hub page and extracts all state list URLs.\"\"\"\n",
        "    state_urls = {}\n",
        "    print(f\"Fetching link hub: {HUB_PAGE_URL}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(HUB_PAGE_URL, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        body_content = soup.find(id=\"bodyContent\")\n",
        "\n",
        "        if body_content:\n",
        "            state_links = body_content.find_all(\n",
        "                'a',\n",
        "                href=lambda href: href and href.startswith('/wiki/List_of_colleges_and_universities_in_')\n",
        "            )\n",
        "            for link in state_links:\n",
        "                state_name = link.get_text(strip=True)\n",
        "                absolute_url = urljoin(BASE_URL, link['href'])\n",
        "                state_urls[state_name] = absolute_url\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching hub page: {e}\")\n",
        "\n",
        "    return state_urls\n",
        "\n",
        "# function to Scrape Colleges from a Single State Page (CLEANED)\n",
        "def scrape_colleges_from_state_page(url, state_name):\n",
        "    \"\"\"Finds the wikitable, reads it with pandas, and extracts the college names.\"\"\"\n",
        "    colleges = []\n",
        "\n",
        "    # using 'School' as the primary match\n",
        "    possible_college_columns = ['School', 'Institution', 'University', 'College', 'Name', 0]\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # using BeautifulSoup to find all tables with the specific Wikipedia class\n",
        "        wiki_tables = soup.find_all('table', {'class': 'wikitable'})\n",
        "\n",
        "        if not wiki_tables:\n",
        "            return colleges\n",
        "\n",
        "        # prioritizing the table with the most rows\n",
        "        best_table = None\n",
        "        best_size = 0\n",
        "        for table in wiki_tables:\n",
        "            row_count = len(table.find_all('tr'))\n",
        "            if row_count > best_size:\n",
        "                best_size = row_count\n",
        "                best_table = table\n",
        "\n",
        "        if not best_table:\n",
        "            return colleges\n",
        "\n",
        "        table_html = str(best_table)\n",
        "\n",
        "        # using pandas.read_html on the SINGLE table's HTML string\n",
        "        data_frames = pd.read_html(StringIO(table_html), header=0)\n",
        "\n",
        "        if not data_frames:\n",
        "             return colleges\n",
        "\n",
        "        df = data_frames[0]\n",
        "\n",
        "        # search the DataFrame's columns for the college name\n",
        "        for col_name in possible_college_columns:\n",
        "            if col_name in df.columns:\n",
        "                names = df[col_name].dropna().astype(str).tolist()\n",
        "                # cleaning by removing bracketed citations like [1], [a], etc.\n",
        "                names = [name.split('[')[0].strip() for name in names]\n",
        "                colleges.extend(names)\n",
        "                return colleges # Success! Exit the function\n",
        "\n",
        "    except Exception: # A broad exception handles network or pandas parsing errors\n",
        "        pass\n",
        "\n",
        "    return colleges\n",
        "\n",
        "# final main execution loop (FULL CRAWL)\n",
        "def run_scraper():\n",
        "    \"\"\"Executes the entire scraping process across all states and saves to CSV.\"\"\"\n",
        "    all_college_data = []\n",
        "\n",
        "    # get all state links\n",
        "    state_links = get_all_state_links()\n",
        "    if not state_links:\n",
        "        print(\"Scraper aborted: Could not get state links.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(state_links)} state/territory links.\")\n",
        "    print(f\"\\nStarting full scraping process for all {len(state_links)} pages...\")\n",
        "\n",
        "    # iterating through each state and scraping its page\n",
        "    for i, (state, url) in enumerate(state_links.items()):\n",
        "        print(f\"  [{i+1}/{len(state_links)}] Scraping colleges in {state}...\")\n",
        "\n",
        "        colleges = scrape_colleges_from_state_page(url, state)\n",
        "\n",
        "        if colleges:\n",
        "            all_college_data.append({\n",
        "                'State': state,\n",
        "                'Colleges': colleges,\n",
        "                'Count': len(colleges)\n",
        "            })\n",
        "            print(f\"    -> Extracted {len(colleges)} institutions.\")\n",
        "        else:\n",
        "            print(f\"    -> No college data extracted for {state}. (Structure may be non-standard)\")\n",
        "\n",
        "    # final Aggregation and Output\n",
        "    total_institutions = sum(item['Count'] for item in all_college_data)\n",
        "\n",
        "    print(\"\\n==============================================\")\n",
        "    print(\"SCRAPING COMPLETE\")\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Total states/territories successfully scraped: {len(all_college_data)} out of {len(state_links)}\")\n",
        "    print(f\"Total institutions collected: {total_institutions:,}\")\n",
        "    print(\"----------------------------------------------\")\n",
        "\n",
        "    # converting to a flat DataFrame and saving the data\n",
        "    all_colleges_flat = []\n",
        "    for item in all_college_data:\n",
        "        for college_name in item['Colleges']:\n",
        "            all_colleges_flat.append({'State': item['State'], 'College Name': college_name})\n",
        "\n",
        "    df_final = pd.DataFrame(all_colleges_flat)\n",
        "\n",
        "    output_filename = 'us_colleges.csv'\n",
        "    print(f\"\\nSaving data to '{output_filename}'...\")\n",
        "    df_final.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "    print(\"Finished. Data saved successfully.\")\n",
        "\n",
        "# executing the final function\n",
        "run_scraper()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the CSV file\n",
        "df = pd.read_csv(\"us_colleges.csv\")\n",
        "\n",
        "# Normalizing columns (remove extra spaces)\n",
        "df['College Name'] = df['College Name'].astype(str).str.strip()\n",
        "df['State'] = df['State'].astype(str).str.strip()\n",
        "\n",
        "# Detecting exact duplicates (same College Name + State)\n",
        "duplicates_mask = df.duplicated(subset=['College Name', 'State'], keep='first')\n",
        "duplicates_df = df[duplicates_mask]\n",
        "\n",
        "# Printing all duplicates that will be removed\n",
        "if not duplicates_df.empty:\n",
        "    print(\"Exact duplicates to be removed:\")\n",
        "    for _, row in duplicates_df.iterrows():\n",
        "        print(f\" - {row['College Name']}  ({row['State']})\")\n",
        "else:\n",
        "    print(\"No exact duplicates found to remove.\")\n",
        "\n",
        "# Dropping only exact duplicates\n",
        "df_clean = df.drop_duplicates(subset=['College Name', 'State'], keep='first')\n",
        "\n",
        "# Sorting\n",
        "df_clean = df_clean.sort_values(by=['State', 'College Name'])\n",
        "\n",
        "# Save cleaned CSV\n",
        "df_clean.to_csv(\"us_colleges_clean.csv\", index=False)\n",
        "\n",
        "# printing for visual purposes\n",
        "print(\"\\n--------------------------------\")\n",
        "print(f\"Before: {len(df)} rows\")\n",
        "print(f\"After:  {len(df_clean)} rows\")\n",
        "print(f\"Removed: {len(df) - len(df_clean)} exact duplicates\")\n",
        "print(\"Clean file saved as 'us_colleges_clean.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--TdJRvdWpg5",
        "outputId": "0a2aae0c-03f4-4b91-a1eb-780f371976b5"
      },
      "id": "--TdJRvdWpg5",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact duplicates to be removed:\n",
            " - United Lutheran Seminary  (Pennsylvania)\n",
            " - Lackawanna College  (Pennsylvania)\n",
            " - Lackawanna College  (Pennsylvania)\n",
            " - Lackawanna College  (Pennsylvania)\n",
            " - Lackawanna College  (Pennsylvania)\n",
            " - Lackawanna College  (Pennsylvania)\n",
            " - Laurel Technical Institute  (Pennsylvania)\n",
            " - McCann School of Business and Technology  (Pennsylvania)\n",
            " - All-State Career School  (Pennsylvania)\n",
            " - Oklahoma State University  (Oklahoma)\n",
            " - California Coast University  (California)\n",
            " - University of Colorado  (Colorado)\n",
            " - University of Colorado  (Colorado)\n",
            " - Jefferson Institute  (Oregon)\n",
            "\n",
            "--------------------------------\n",
            "Before: 2090 rows\n",
            "After:  2076 rows\n",
            "Removed: 14 exact duplicates\n",
            "Clean file saved as 'us_colleges_clean.csv'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}