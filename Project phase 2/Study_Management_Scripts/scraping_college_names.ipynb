{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe963be4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# COLLECTING COLLEGES NAMES AND STATES\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "from io import StringIO\n",
    "import warnings\n",
    "\n",
    "# suppress pandas warning about SSL/certificate checks\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# configurations\n",
    "BASE_URL = \"https://en.wikipedia.org\"\n",
    "HUB_PAGE_URL = \"https://en.wikipedia.org/wiki/Lists_of_American_universities_and_colleges\"\n",
    "\n",
    "headers = {\n",
    "    # using a standard User-Agent to bypass the 403 error\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# function to Get All State Links\n",
    "def get_all_state_links():\n",
    "    \"\"\"Fetches the hub page and extracts all state list URLs.\"\"\"\n",
    "    state_urls = {}\n",
    "    print(f\"Fetching link hub: {HUB_PAGE_URL}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(HUB_PAGE_URL, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        body_content = soup.find(id=\"bodyContent\")\n",
    "\n",
    "        if body_content:\n",
    "            state_links = body_content.find_all(\n",
    "                'a',\n",
    "                href=lambda href: href and href.startswith('/wiki/List_of_colleges_and_universities_in_')\n",
    "            )\n",
    "            for link in state_links:\n",
    "                state_name = link.get_text(strip=True)\n",
    "                absolute_url = urljoin(BASE_URL, link['href'])\n",
    "                state_urls[state_name] = absolute_url\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching hub page: {e}\")\n",
    "\n",
    "    return state_urls\n",
    "\n",
    "# function to Scrape Colleges from a Single State Page (CLEANED)\n",
    "def scrape_colleges_from_state_page(url, state_name):\n",
    "    \"\"\"Finds the wikitable, reads it with pandas, and extracts the college names.\"\"\"\n",
    "    colleges = []\n",
    "\n",
    "    # using 'School' as the primary match\n",
    "    possible_college_columns = ['School', 'Institution', 'University', 'College', 'Name', 0]\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # using BeautifulSoup to find all tables with the specific Wikipedia class\n",
    "        wiki_tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "\n",
    "        if not wiki_tables:\n",
    "            return colleges\n",
    "\n",
    "        # prioritizing the table with the most rows\n",
    "        best_table = None\n",
    "        best_size = 0\n",
    "        for table in wiki_tables:\n",
    "            row_count = len(table.find_all('tr'))\n",
    "            if row_count > best_size:\n",
    "                best_size = row_count\n",
    "                best_table = table\n",
    "\n",
    "        if not best_table:\n",
    "            return colleges\n",
    "\n",
    "        table_html = str(best_table)\n",
    "\n",
    "        # using pandas.read_html on the SINGLE table's HTML string\n",
    "        data_frames = pd.read_html(StringIO(table_html), header=0)\n",
    "\n",
    "        if not data_frames:\n",
    "             return colleges\n",
    "\n",
    "        df = data_frames[0]\n",
    "\n",
    "        # search the DataFrame's columns for the college name\n",
    "        for col_name in possible_college_columns:\n",
    "            if col_name in df.columns:\n",
    "                names = df[col_name].dropna().astype(str).tolist()\n",
    "                # cleaning by removing bracketed citations like [1], [a], etc.\n",
    "                names = [name.split('[')[0].strip() for name in names]\n",
    "                colleges.extend(names)\n",
    "                return colleges # Success! Exit the function\n",
    "\n",
    "    except Exception: # A broad exception handles network or pandas parsing errors\n",
    "        pass\n",
    "\n",
    "    return colleges\n",
    "\n",
    "# final Main Execution Loop (FULL CRAWL)\n",
    "def run_scraper():\n",
    "    \"\"\"Executes the entire scraping process across all states and saves to CSV.\"\"\"\n",
    "    all_college_data = []\n",
    "\n",
    "    # get all state links\n",
    "    state_links = get_all_state_links()\n",
    "    if not state_links:\n",
    "        print(\"Scraper aborted: Could not get state links.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(state_links)} state/territory links.\")\n",
    "    print(f\"\\nStarting full scraping process for all {len(state_links)} pages...\")\n",
    "\n",
    "    # iterating through each state and scraping its page\n",
    "    for i, (state, url) in enumerate(state_links.items()):\n",
    "        print(f\"  [{i+1}/{len(state_links)}] Scraping colleges in {state}...\")\n",
    "\n",
    "        colleges = scrape_colleges_from_state_page(url, state)\n",
    "\n",
    "        if colleges:\n",
    "            all_college_data.append({\n",
    "                'State': state,\n",
    "                'Colleges': colleges,\n",
    "                'Count': len(colleges)\n",
    "            })\n",
    "            print(f\"    -> Extracted {len(colleges)} institutions.\")\n",
    "        else:\n",
    "            print(f\"    -> No college data extracted for {state}. (Structure may be non-standard)\")\n",
    "\n",
    "    # final Aggregation and Output\n",
    "    total_institutions = sum(item['Count'] for item in all_college_data)\n",
    "\n",
    "    print(\"\\n==============================================\")\n",
    "    print(\"SCRAPING COMPLETE\")\n",
    "    print(\"==============================================\")\n",
    "    print(f\"Total states/territories successfully scraped: {len(all_college_data)} out of {len(state_links)}\")\n",
    "    print(f\"Total institutions collected: {total_institutions:,}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    # converting to a flat DataFrame and saving the data\n",
    "    all_colleges_flat = []\n",
    "    for item in all_college_data:\n",
    "        for college_name in item['Colleges']:\n",
    "            all_colleges_flat.append({'State': item['State'], 'College Name': college_name})\n",
    "\n",
    "    df_final = pd.DataFrame(all_colleges_flat)\n",
    "\n",
    "    output_filename = 'us_colleges.csv'\n",
    "    print(f\"\\nSaving data to '{output_filename}'...\")\n",
    "    df_final.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "    print(\"Finished. Data saved successfully.\")\n",
    "\n",
    "# executing the final function\n",
    "run_scraper()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
