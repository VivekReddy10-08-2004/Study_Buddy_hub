{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e8db02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Project path added.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "# Tell Python to also look one folder up (where 'db' lives)\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# Optional: confirm it worked\n",
    "print(\" Project path added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0635a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1699 courses, 100 users, 10 topic types\n",
      " Saved 100 topics \n",
      " Inserted 100 rows into topics table.\n"
     ]
    }
   ],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize\n",
    "fake = Faker()\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Fetch existing foreign keys\n",
    "cursor.execute(\"SELECT course_id FROM Courses;\")\n",
    "course_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.execute(\"SELECT user_id FROM Users;\")\n",
    "user_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.execute(\"SELECT topic_type_id FROM topic_types;\")\n",
    "topic_type_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(f\" Loaded {len(course_ids)} courses, {len(user_ids)} users, {len(topic_type_ids)} topic types\")\n",
    "\n",
    "# Generate 100 random topics\n",
    "topics = []\n",
    "for _ in range(100):\n",
    "    course_id = random.choice(course_ids)\n",
    "    user_id = random.choice(user_ids)\n",
    "    topic_type_id = random.choice(topic_type_ids)\n",
    "    topic_name = fake.sentence(nb_words=random.randint(2, 5)).replace(\".\", \"\")\n",
    "    topics.append((course_id, user_id, topic_name, topic_type_id))\n",
    "\n",
    "# Save CSV one level up\n",
    "output_dir = Path(\"..\") / \"data\" / \"Raw_data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = output_dir / \"topics.csv\"\n",
    "\n",
    "df_topics = pd.DataFrame(topics, columns=[\"course_id\", \"user_id\", \"topic_name\", \"topic_type_id\"])\n",
    "df_topics.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" Saved 100 topics \")\n",
    "\n",
    "# Insert generated topics into the database\n",
    "df = pd.read_csv(csv_path) \n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO topics (course_id, user_id, topic_name, topic_type_id)\n",
    "VALUES (%s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "count = 0\n",
    "for _, row in df.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "    count += 1\n",
    "\n",
    "connection.commit()\n",
    "print(f\" Inserted {count} rows into topics table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b260ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Issue', 1650, 55, 6, datetime.datetime(2025, 11, 15, 2, 16, 4))\n",
      "(2, 'Us thought', 1275, 10, 8, datetime.datetime(2025, 11, 15, 2, 16, 4))\n",
      "(3, 'Yet rock compare', 59, 39, 8, datetime.datetime(2025, 11, 15, 2, 16, 4))\n",
      "(4, 'Reveal phone never', 1446, 96, 8, datetime.datetime(2025, 11, 15, 2, 16, 4))\n",
      "(5, 'Talk so', 163, 48, 10, datetime.datetime(2025, 11, 15, 2, 16, 4))\n",
      "(6, 'Growth political after wait door', 640, 100, 7, datetime.datetime(2025, 11, 15, 2, 16, 4))\n",
      "(7, 'Find base actually', 1682, 88, 1, datetime.datetime(2025, 11, 15, 2, 16, 4))\n",
      "(8, 'Old view home', 876, 97, 8, datetime.datetime(2025, 11, 15, 2, 16, 4))\n",
      "(9, 'Wonder just forget', 467, 86, 9, datetime.datetime(2025, 11, 15, 2, 16, 4))\n",
      "(10, 'Try perform', 1369, 45, 5, datetime.datetime(2025, 11, 15, 2, 16, 4))\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT topic_id, topic_name, course_id, user_id, topic_type_id, created_at FROM topics LIMIT 10;\")\n",
    "for row in cursor.fetchall():\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25882095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize\n",
    "fake = Faker()\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Fetch existing foreign keys\n",
    "cursor.execute(\"SELECT user_id FROM Users;\")\n",
    "user_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.execute(\"SELECT course_id FROM Courses;\")\n",
    "course_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.execute(\"SELECT topic_id FROM topics;\")\n",
    "topic_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.execute(\"SELECT category_id FROM task_categories;\")\n",
    "category_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(f\"ðŸ“Š Loaded {len(user_ids)} users, {len(course_ids)} courses, {len(topic_ids)} topics, {len(category_ids)} categories\")\n",
    "\n",
    "# Generate 100 random tasks\n",
    "statuses = ['todo', 'in_progress', 'done']\n",
    "tasks = []\n",
    "\n",
    "for _ in range(100):\n",
    "    user_id = random.choice(user_ids)\n",
    "    course_id = random.choice(course_ids)\n",
    "    topic_id = random.choice(topic_ids)\n",
    "    category_id = random.choice(category_ids)\n",
    "\n",
    "    title = fake.sentence(nb_words=random.randint(3, 6)).replace(\".\", \"\")\n",
    "    due_date = datetime.now() + timedelta(days=random.randint(1, 30))\n",
    "    status = random.choice(statuses)\n",
    "    priority = random.randint(1, 5)\n",
    "\n",
    "    tasks.append((user_id, course_id, topic_id, title, due_date, status, priority, category_id))\n",
    "\n",
    "# Save to ../data/Raw_data/\n",
    "output_dir = Path(\"..\") / \"data\" / \"Raw_data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = output_dir / \"tasks.csv\"\n",
    "\n",
    "df_tasks = pd.DataFrame(tasks, columns=[\n",
    "    \"user_id\", \"course_id\", \"topic_id\", \"title\", \"due_date\", \"status\", \"priority\", \"category_id\"\n",
    "])\n",
    "df_tasks.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" Saved 100 tasks\")\n",
    "\n",
    "# Insert into the database\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO tasks (user_id, course_id, topic_id, title, due_date, status, priority, category_id)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "count = 0\n",
    "for _, row in df_tasks.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "    count += 1\n",
    "\n",
    "connection.commit()\n",
    "print(f\" Inserted {count} rows into tasks table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d20823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Power policy wrong teacher very agency democratic performance', 'todo', datetime.datetime(2025, 11, 19, 2, 20, 10), 5)\n",
      "(2, 'That with care', 'done', datetime.datetime(2025, 12, 3, 2, 20, 10), 1)\n",
      "(3, 'Production per new watch', 'todo', datetime.datetime(2025, 11, 27, 2, 20, 10), 1)\n",
      "(4, 'Road total', 'done', datetime.datetime(2025, 12, 14, 2, 20, 10), 2)\n",
      "(5, 'Upon know to military tell when', 'done', datetime.datetime(2025, 12, 14, 2, 20, 10), 5)\n",
      "(6, 'Choice region glass', 'todo', datetime.datetime(2025, 11, 28, 2, 20, 10), 5)\n",
      "(7, 'Once art interview moment senior collection', 'done', datetime.datetime(2025, 11, 30, 2, 20, 10), 3)\n",
      "(8, 'Charge onto fear', 'todo', datetime.datetime(2025, 11, 22, 2, 20, 10), 1)\n",
      "(9, 'Entire boy', 'todo', datetime.datetime(2025, 11, 20, 2, 20, 10), 5)\n",
      "(10, 'Window effort their say whole', 'todo', datetime.datetime(2025, 11, 20, 2, 20, 10), 2)\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT task_id, title, status, due_date, priority FROM tasks LIMIT 10;\")\n",
    "for row in cursor.fetchall():\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b3c06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL connector is working!\n",
      " Loaded 100 users and 100 topics.\n",
      " Saved 100 timer sessions\n",
      " Inserted 100 rows into timersessions table.\n"
     ]
    }
   ],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize\n",
    "fake = Faker()\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# --- Fetch foreign keys\n",
    "cursor.execute(\"SELECT user_id FROM Users;\")\n",
    "user_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.execute(\"SELECT topic_id FROM topics;\")\n",
    "topic_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(f\" Loaded {len(user_ids)} users and {len(topic_ids)} topics.\")\n",
    "\n",
    "# --- Generate 100 focus sessions\n",
    "techniques = [\"pomodoro\", \"flowtime\", \"custom\"]\n",
    "session_types = [\"solo\", \"group\"]\n",
    "sessions = []\n",
    "\n",
    "for _ in range(100):\n",
    "    host_id = random.choice(user_ids)\n",
    "    topic_id = random.choice(topic_ids)\n",
    "    technique_type = random.choice(techniques)\n",
    "    session_type = random.choice(session_types)\n",
    "\n",
    "    # Random start time within the past 30 days\n",
    "    start_time = datetime.now() - timedelta(days=random.randint(0, 30),\n",
    "                                            hours=random.randint(0, 23),\n",
    "                                            minutes=random.randint(0, 59))\n",
    "    duration = random.randint(15, 120)   # minutes\n",
    "    end_time = start_time + timedelta(minutes=duration)\n",
    "\n",
    "    # Breaks (short 5-10 min, long 15-25 min)\n",
    "    short_break = random.choice([5, 10, 15])\n",
    "    long_break = random.choice([15, 20, 25])\n",
    "\n",
    "    sessions.append((\n",
    "        host_id,\n",
    "        start_time,\n",
    "        end_time,\n",
    "        topic_id,\n",
    "        technique_type,\n",
    "        short_break,\n",
    "        long_break,\n",
    "        session_type\n",
    "    ))\n",
    "\n",
    "# --- Save CSV one level up â†’ ../data/Raw_data/\n",
    "output_dir = Path(\"..\") / \"data\" / \"Raw_data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = output_dir / \"timersessions.csv\"\n",
    "\n",
    "df_sessions = pd.DataFrame(\n",
    "    sessions,\n",
    "    columns=[\n",
    "        \"host_id\",\n",
    "        \"start_time\",\n",
    "        \"end_time\",\n",
    "        \"topic_id\",\n",
    "        \"technique_type\",\n",
    "        \"short_break_min\",\n",
    "        \"long_break_min\",\n",
    "        \"session_type\"\n",
    "    ]\n",
    ")\n",
    "df_sessions.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" Saved 100 timer sessions\")\n",
    "\n",
    "# --- Insert into the database\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO timersessions (\n",
    "    host_id, start_time, end_time, topic_id,\n",
    "    technique_type, short_break_min, long_break_min, session_type\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "count = 0\n",
    "for _, row in df_sessions.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "    count += 1\n",
    "\n",
    "connection.commit()\n",
    "print(f\" Inserted {count} rows into timersessions table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "531a34f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 timer sessions and 100 users.\n",
      "Saved 100 session participants.\n",
      "Inserted 100 rows into sessionparticipants table.\n"
     ]
    }
   ],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize\n",
    "fake = Faker()\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Fetch existing foreign keys\n",
    "cursor.execute(\"SELECT timer_id FROM timersessions;\")\n",
    "timer_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.execute(\"SELECT user_id FROM Users;\")\n",
    "user_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(f\"Loaded {len(timer_ids)} timer sessions and {len(user_ids)} users.\")\n",
    "\n",
    "# Generate 100 random participant entries\n",
    "records = set()  # prevent duplicates\n",
    "participants = []\n",
    "\n",
    "while len(participants) < 100:\n",
    "    timer_id = random.choice(timer_ids)\n",
    "    user_id = random.choice(user_ids)\n",
    "    if (timer_id, user_id) in records:\n",
    "        continue  # avoid duplicate primary key\n",
    "    records.add((timer_id, user_id))\n",
    "    role = random.choice(['host', 'member'])\n",
    "    joined_at = fake.date_time_this_year()\n",
    "    participants.append((timer_id, user_id, joined_at, role))\n",
    "\n",
    "# Save to CSV one level up\n",
    "output_dir = Path(\"..\") / \"data\" / \"Raw_data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = output_dir / \"sessionparticipants.csv\"\n",
    "\n",
    "df_participants = pd.DataFrame(participants, columns=[\"timer_id\", \"user_id\", \"joined_at\", \"role\"])\n",
    "df_participants.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved {len(df_participants)} session participants.\")\n",
    "\n",
    "# Insert into DB\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO sessionparticipants (timer_id, user_id, joined_at, role)\n",
    "VALUES (%s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "count = 0\n",
    "for _, row in df_participants.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "    count += 1\n",
    "\n",
    "connection.commit()\n",
    "print(f\"Inserted {count} rows into sessionparticipants table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bd24c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 tasks.\n",
      "Saved 100 reminders.\n",
      "Inserted 100 rows into reminders table.\n"
     ]
    }
   ],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "# Initialize\n",
    "fake = Faker()\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Fetch task IDs\n",
    "cursor.execute(\"SELECT task_id FROM tasks;\")\n",
    "task_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(f\"Loaded {len(task_ids)} tasks.\")\n",
    "\n",
    "# Generate 100 reminders\n",
    "reminders = []\n",
    "for _ in range(100):\n",
    "    task_id = random.choice(task_ids)\n",
    "    # reminder_time: within next 30 days\n",
    "    reminder_time = fake.date_time_between(start_date=\"now\", end_date=\"+30d\")\n",
    "    method = random.choice(['in_app', 'email', 'sms'])\n",
    "    reminders.append((task_id, reminder_time, method))\n",
    "\n",
    "# Save CSV one level up\n",
    "output_dir = Path(\"..\") / \"data\" / \"Raw_data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = output_dir / \"reminders.csv\"\n",
    "\n",
    "df_reminders = pd.DataFrame(reminders, columns=[\"task_id\", \"reminder_time\", \"method\"])\n",
    "df_reminders.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved {len(df_reminders)} reminders.\")\n",
    "\n",
    "# Insert into DB\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO reminders (task_id, reminder_time, method)\n",
    "VALUES (%s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "count = 0\n",
    "for _, row in df_reminders.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "    count += 1\n",
    "\n",
    "connection.commit()\n",
    "print(f\"Inserted {count} rows into reminders table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d313a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 item types.\n",
      "Saved 100 focus items\n",
      "Inserted 100 rows into focusitems table.\n"
     ]
    }
   ],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize\n",
    "fake = Faker()\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Fetch existing item types\n",
    "cursor.execute(\"SELECT item_type_id, type_name FROM item_types;\")\n",
    "item_types = cursor.fetchall()\n",
    "\n",
    "print(f\"Loaded {len(item_types)} item types.\")\n",
    "\n",
    "# Helper data\n",
    "rarities = ['common', 'rare', 'epic', 'legendary']\n",
    "\n",
    "# Generate 100 focus items\n",
    "focus_items = []\n",
    "for _ in range(100):\n",
    "    item_type_id, type_name = random.choice(item_types)\n",
    "    item_name = f\"{fake.word().capitalize()} {type_name}\"\n",
    "    image_url = f\"https://example.com/images/{item_name.replace(' ', '_').lower()}.png\"\n",
    "    rarity = random.choices(rarities, weights=[60, 25, 10, 5])[0]  # weighted probabilities\n",
    "    focus_cost_min = random.randint(50, 500)\n",
    "    description = fake.sentence(nb_words=6).replace(\".\", \"\")\n",
    "    focus_items.append((item_type_id, item_name, image_url, rarity, focus_cost_min, description))\n",
    "\n",
    "# Save to CSV \n",
    "output_dir = Path(\"..\") / \"data\" / \"Raw_data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = output_dir / \"focusitems.csv\"\n",
    "\n",
    "df_items = pd.DataFrame(focus_items, columns=[\n",
    "    \"item_type_id\", \"item_name\", \"image_url\", \"rarity_level\", \"focus_cost_min\", \"description\"\n",
    "])\n",
    "df_items.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved {len(df_items)} focus items\")\n",
    "\n",
    "# --- Insert into DB\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO focusitems (item_type_id, item_name, image_url, rarity_level, focus_cost_min, description)\n",
    "VALUES (%s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "count = 0\n",
    "for _, row in df_items.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "    count += 1\n",
    "\n",
    "connection.commit()\n",
    "print(f\"Inserted {count} rows into focusitems table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a9f0edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 100 users and 100 focus items.\n",
      "\n",
      " NaN counts per column:\n",
      " user_id              0\n",
      "focus_date           0\n",
      "total_sessions       0\n",
      "total_focus_min      0\n",
      "focus_start_times    0\n",
      "focus_topics         0\n",
      "item_earned_id       0\n",
      "dtype: int64\n",
      "\n",
      " Saved 100 clean daily focus logs.\n",
      " Inserted 100 rows into dailyfocuslog table successfully.\n",
      " Database connection closed.\n",
      "\n",
      " Daily focus log generation complete.\n"
     ]
    }
   ],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "# Initialize Faker and DB connection\n",
    "fake = Faker()\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Fetch existing foreign key data (Users + Focus Items)\n",
    "cursor.execute(\"SELECT user_id FROM Users;\")\n",
    "user_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.execute(\"SELECT item_id FROM focusitems;\")\n",
    "item_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(f\" Loaded {len(user_ids)} users and {len(item_ids)} focus items.\")\n",
    "\n",
    "# Generate Daily Focus Logs\n",
    "logs = []\n",
    "for _ in range(100):\n",
    "    user_id = random.choice(user_ids)\n",
    "    focus_date = fake.date_between(start_date=\"-30d\", end_date=\"today\")\n",
    "\n",
    "    total_sessions = random.randint(1, 8)\n",
    "    total_focus_min = total_sessions * random.randint(20, 60)\n",
    "\n",
    "    # Generate JSON fields (start times + topics)\n",
    "    focus_start_times = [fake.time() for _ in range(total_sessions)]\n",
    "    focus_topics = [fake.word() for _ in range(random.randint(2, 4))]\n",
    "\n",
    "    # Convert to JSON-safe strings\n",
    "    focus_start_times_json = json.dumps(focus_start_times, ensure_ascii=False)\n",
    "    focus_topics_json = json.dumps(focus_topics, ensure_ascii=False)\n",
    "\n",
    "    # Reward system (like Forest app):\n",
    "    # Users earn better items for longer total focus minutes\n",
    "    if total_focus_min < 60:\n",
    "        item_earned_id = random.choice(item_ids[:25]) if len(item_ids) >= 25 else random.choice(item_ids)\n",
    "    elif total_focus_min < 180:\n",
    "        item_earned_id = random.choice(item_ids[25:50]) if len(item_ids) >= 50 else random.choice(item_ids)\n",
    "    elif total_focus_min < 360:\n",
    "        item_earned_id = random.choice(item_ids[50:75]) if len(item_ids) >= 75 else random.choice(item_ids)\n",
    "    else:\n",
    "        item_earned_id = random.choice(item_ids[75:]) if len(item_ids) > 75 else random.choice(item_ids)\n",
    "\n",
    "    logs.append((\n",
    "        user_id,\n",
    "        focus_date,\n",
    "        total_sessions,\n",
    "        total_focus_min,\n",
    "        focus_start_times_json,\n",
    "        focus_topics_json,\n",
    "        item_earned_id\n",
    "    ))\n",
    "\n",
    "# Save as CSV (for backup and inspection)\n",
    "output_dir = Path(\"..\") / \"data\" / \"Raw_data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = output_dir / \"dailyfocuslog.csv\"\n",
    "\n",
    "df_logs = pd.DataFrame(logs, columns=[\n",
    "    \"user_id\", \"focus_date\", \"total_sessions\", \"total_focus_min\",\n",
    "    \"focus_start_times\", \"focus_topics\", \"item_earned_id\"\n",
    "])\n",
    "\n",
    "# Validate data before inserting\n",
    "nan_summary = df_logs.isna().sum()\n",
    "print(\"\\n NaN counts per column:\\n\", nan_summary)\n",
    "\n",
    "assert not df_logs.isna().any().any(), \" DataFrame contains NaN values!\"\n",
    "assert df_logs[\"focus_start_times\"].str.len().min() > 0, \" Empty JSON strings found!\"\n",
    "\n",
    "df_logs.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\n Saved {len(df_logs)} clean daily focus logs.\")\n",
    "\n",
    "# Insert into MySQL database\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO dailyfocuslog \n",
    "(user_id, focus_date, total_sessions, total_focus_min, focus_start_times, focus_topics, item_earned_id)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "count = 0\n",
    "for _, row in df_logs.iterrows():\n",
    "    cursor.execute(insert_query, tuple(row))\n",
    "    count += 1\n",
    "\n",
    "connection.commit()\n",
    "print(f\" Inserted {count} rows into dailyfocuslog table successfully.\")\n",
    "\n",
    "# Clean up\n",
    "cursor.close()\n",
    "connection.close()\n",
    "print(\" Database connection closed.\\n\")\n",
    "print(\" Daily focus log generation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcdc2069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 daily focus records from database.\n",
      "Loaded 100 focus items with their types.\n",
      " Inserted or updated 68 user study stats successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xv/rz8bm0912bn7rcqqwsgsxtsc0000gn/T/ipykernel_80133/1231412668.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "\n",
    "# Connect to database\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Load daily focus logs\n",
    "query = \"\"\"\n",
    "SELECT user_id, focus_date, total_sessions, total_focus_min, item_earned_id, focus_topics\n",
    "FROM dailyfocuslog;\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, connection)\n",
    "print(f\"Loaded {len(df)} daily focus records from database.\")\n",
    "\n",
    "# Load item and type info\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT fi.item_id, fi.item_name, it.type_name\n",
    "    FROM focusitems fi\n",
    "    JOIN item_types it ON fi.item_type_id = it.item_type_id;\n",
    "\"\"\")\n",
    "item_info = {row[0]: {\"name\": row[1], \"type\": row[2]} for row in cursor.fetchall()}\n",
    "print(f\"Loaded {len(item_info)} focus items with their types.\")\n",
    "\n",
    "# Load topic mapping (assuming topics table exists)\n",
    "cursor.execute(\"SELECT topic_id, topic_name FROM topics;\")\n",
    "topic_map = {name: tid for tid, name in cursor.fetchall()}\n",
    "\n",
    "# Compute stats per user\n",
    "stats = []\n",
    "for user_id, group in df.groupby(\"user_id\"):\n",
    "    total_sessions = int(group[\"total_sessions\"].sum())\n",
    "    total_focus_time_min = int(group[\"total_focus_min\"].sum())\n",
    "    avg_duration_min = round(total_focus_time_min / total_sessions, 2) if total_sessions else 0.0\n",
    "\n",
    "    # Streaks\n",
    "    dates = sorted(pd.to_datetime(group[\"focus_date\"]).unique())\n",
    "    longest_streak = current_streak = 1\n",
    "    for i in range(1, len(dates)):\n",
    "        if (dates[i] - dates[i - 1]).days == 1:\n",
    "            current_streak += 1\n",
    "            longest_streak = max(longest_streak, current_streak)\n",
    "        else:\n",
    "            current_streak = 1\n",
    "\n",
    "    last_session_at = pd.to_datetime(group[\"focus_date\"]).max()\n",
    "\n",
    "    # Favorite item\n",
    "    most_common_item_id = (\n",
    "        int(group[\"item_earned_id\"].mode().iloc[0])\n",
    "        if not group[\"item_earned_id\"].isna().all()\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    favorite_item_type = (\n",
    "        item_info[most_common_item_id][\"type\"]\n",
    "        if most_common_item_id in item_info\n",
    "        else None\n",
    "    )\n",
    "    favorite_item_name = (\n",
    "        item_info[most_common_item_id][\"name\"]\n",
    "        if most_common_item_id in item_info\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Most frequent topic\n",
    "    all_topics = []\n",
    "    for tlist in group[\"focus_topics\"]:\n",
    "        try:\n",
    "            topics = eval(tlist) if isinstance(tlist, str) else []\n",
    "            all_topics.extend(topics)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    most_common_topic_name = (\n",
    "        Counter(all_topics).most_common(1)[0][0] if all_topics else None\n",
    "    )\n",
    "    most_frequent_topic = (\n",
    "        int(topic_map.get(most_common_topic_name))\n",
    "        if most_common_topic_name in topic_map\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    stats.append((\n",
    "        int(user_id),\n",
    "        total_sessions,\n",
    "        total_focus_time_min,\n",
    "        float(avg_duration_min),\n",
    "        int(longest_streak),\n",
    "        int(current_streak),\n",
    "        last_session_at.to_pydatetime() if pd.notna(last_session_at) else None,\n",
    "        str(favorite_item_type) if favorite_item_type else None,\n",
    "        str(favorite_item_name) if favorite_item_name else None,\n",
    "        most_frequent_topic\n",
    "    ))\n",
    "\n",
    "# Insert results into `studystats`\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO studystats (\n",
    "    user_id, total_sessions, total_focus_time_min, avg_duration_min,\n",
    "    longest_streak_days, current_streak_days, last_session_at,\n",
    "    favorite_item_type, favorite_item_name, most_frequent_topic\n",
    ")\n",
    "VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "ON DUPLICATE KEY UPDATE\n",
    "    total_sessions=VALUES(total_sessions),\n",
    "    total_focus_time_min=VALUES(total_focus_time_min),\n",
    "    avg_duration_min=VALUES(avg_duration_min),\n",
    "    longest_streak_days=VALUES(longest_streak_days),\n",
    "    current_streak_days=VALUES(current_streak_days),\n",
    "    last_session_at=VALUES(last_session_at),\n",
    "    favorite_item_type=VALUES(favorite_item_type),\n",
    "    favorite_item_name=VALUES(favorite_item_name),\n",
    "    most_frequent_topic=VALUES(most_frequent_topic);\n",
    "\"\"\"\n",
    "\n",
    "cursor.executemany(insert_query, stats)\n",
    "connection.commit()\n",
    "\n",
    "print(f\" Inserted or updated {len(stats)} user study stats successfully.\")\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "783ef83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 99 earned item records from dailyfocuslog.\n",
      " Inserted or updated 99 user inventory records successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xv/rz8bm0912bn7rcqqwsgsxtsc0000gn/T/ipykernel_80133/480602331.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Fetch earned items from dailyfocuslog\n",
    "query = \"\"\"\n",
    "SELECT user_id, item_earned_id, MAX(focus_date) AS last_date, COUNT(*) AS earned_count\n",
    "FROM dailyfocuslog\n",
    "WHERE item_earned_id IS NOT NULL\n",
    "GROUP BY user_id, item_earned_id;\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, connection)\n",
    "print(f\"Loaded {len(df)} earned item records from dailyfocuslog.\")\n",
    "\n",
    "# Prepare data for insertion\n",
    "records = []\n",
    "for _, row in df.iterrows():\n",
    "    user_id = int(row[\"user_id\"])\n",
    "    item_id = int(row[\"item_earned_id\"])\n",
    "    quantity = int(row[\"earned_count\"])\n",
    "    last_earned_at = pd.to_datetime(row[\"last_date\"]).to_pydatetime()\n",
    "\n",
    "    records.append((user_id, item_id, quantity, last_earned_at, False))\n",
    "\n",
    "# Insert or update into userfocusitems\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO userfocusitems (user_id, item_id, quantity, last_earned_at, is_placed)\n",
    "VALUES (%s, %s, %s, %s, %s)\n",
    "ON DUPLICATE KEY UPDATE\n",
    "    quantity = quantity + VALUES(quantity),\n",
    "    last_earned_at = GREATEST(last_earned_at, VALUES(last_earned_at));\n",
    "\"\"\"\n",
    "\n",
    "cursor.executemany(insert_query, records)\n",
    "connection.commit()\n",
    "\n",
    "print(f\" Inserted or updated {len(records)} user inventory records successfully.\")\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3298d2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 99 userâ€“item ownership records.\n",
      "Generated 100 total placements for user city layouts.\n",
      " Saved 100 layout records to usercitylayout.csv.\n",
      " Inserted 100 rows into usercitylayout successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xv/rz8bm0912bn7rcqqwsgsxtsc0000gn/T/ipykernel_80133/2233827405.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Load user-owned items (from userfocusitems)\n",
    "query = \"\"\"\n",
    "SELECT user_id, item_id, quantity\n",
    "FROM userfocusitems;\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "print(f\"Loaded {len(df)} userâ€“item ownership records.\")\n",
    "\n",
    "# Generate layout placements\n",
    "placements = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    user_id = int(row[\"user_id\"])\n",
    "    item_id = int(row[\"item_id\"])\n",
    "    quantity = int(row[\"quantity\"])\n",
    "\n",
    "    # Decide how many items to place (at least 1, at most quantity)\n",
    "    num_places = max(1, min(quantity, random.randint(1, quantity)))\n",
    "\n",
    "    for _ in range(num_places):\n",
    "        position_x = random.randint(0, 50)  # example grid width\n",
    "        position_y = random.randint(0, 50)  # example grid height\n",
    "\n",
    "        placements.append((\n",
    "            user_id,\n",
    "            item_id,\n",
    "            position_x,\n",
    "            position_y,\n",
    "            datetime.now()\n",
    "        ))\n",
    "\n",
    "print(f\"Generated {len(placements)} total placements for user city layouts.\")\n",
    "\n",
    "# Save CSV to ../data/Raw_data/\n",
    "output_dir = Path(\"..\") / \"data\" / \"Raw_data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = output_dir / \"usercitylayout.csv\"\n",
    "\n",
    "df_place = pd.DataFrame(placements, columns=[\n",
    "    \"user_id\", \"item_id\", \"position_x\", \"position_y\", \"placed_at\"\n",
    "])\n",
    "df_place.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\" Saved {len(df_place)} layout records to usercitylayout.csv.\")\n",
    "\n",
    "# Insert into database\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO usercitylayout (user_id, item_id, position_x, position_y, placed_at)\n",
    "VALUES (%s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "cursor.executemany(insert_query, placements)\n",
    "connection.commit()\n",
    "\n",
    "print(f\" Inserted {len(placements)} rows into usercitylayout successfully.\")\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1c2bd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 reward items.\n",
      "\n",
      "Valid rows: 100 | Bad rows skipped: 0\n",
      "\n",
      "CSV saved.\n",
      "\n",
      "Inserted 100 rows successfully.\n",
      "Failed rows: 0\n"
     ]
    }
   ],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "fake = Faker()\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Fetch reward items\n",
    "cursor.execute(\"SELECT item_id FROM focusitems;\")\n",
    "item_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(f\"Loaded {len(item_ids)} reward items.\")\n",
    "\n",
    "# Allowed chars only\n",
    "allowed_regex = re.compile(r\"^[A-Za-z0-9 ,.\\-?!()]{3,150}$\")\n",
    "strip_illegal = re.compile(r\"[^A-Za-z0-9 ,.\\-?!()]\")\n",
    "\n",
    "def clean_ascii(text):\n",
    "    \"\"\"Remove unicode, enforce allowed characters.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode()\n",
    "    text = strip_illegal.sub(\"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if len(text) < 3:\n",
    "        text = \"Focus Challenge\"\n",
    "    return text[:140]\n",
    "\n",
    "keywords = [\n",
    "    \"Focus Blitz\", \"Study Marathon\", \"Brain Boost\", \"Zen Challenge\",\n",
    "    \"Deep Work Sprint\", \"No Distractions\", \"Knowledge Quest\",\n",
    "    \"Learning Rush\", \"Mind Growth\", \"Pomodoro Push\",\n",
    "    \"Exam Prep Surge\", \"Discipline Drive\", \"Mastery Month\"\n",
    "]\n",
    "\n",
    "def random_month_range():\n",
    "    start = fake.date_between(start_date=\"-6M\", end_date=\"+1M\")\n",
    "    end = start + timedelta(days=random.randint(28, 35))\n",
    "    return start, end\n",
    "\n",
    "rows = []\n",
    "bad_rows = []\n",
    "\n",
    "for _ in range(100):\n",
    "    raw_title = f\"{random.choice(keywords)} {random.randint(1, 300)}\"\n",
    "    title = clean_ascii(raw_title)\n",
    "\n",
    "    # Force validity\n",
    "    if not allowed_regex.match(title):\n",
    "        title = f\"Focus Challenge {random.randint(1,999)}\"\n",
    "\n",
    "    description = clean_ascii(fake.sentence(nb_words=10).replace(\".\", \"\"))\n",
    "\n",
    "    start_date, end_date = random_month_range()\n",
    "    reward_item_id = random.choice(item_ids)\n",
    "\n",
    "    # Validate BEFORE writing to DB\n",
    "    if not allowed_regex.match(title):\n",
    "        bad_rows.append((\"TITLE\", title))\n",
    "        continue\n",
    "    if len(description) > 255:\n",
    "        bad_rows.append((\"DESC_TOO_LONG\", description))\n",
    "        continue\n",
    "\n",
    "    rows.append((title, description, start_date, end_date, 600, reward_item_id))\n",
    "\n",
    "print(f\"\\nValid rows: {len(rows)} | Bad rows skipped: {len(bad_rows)}\")\n",
    "if bad_rows:\n",
    "    print(\"Examples of bad rows:\", bad_rows[:5])\n",
    "\n",
    "# Save CSV\n",
    "output_dir = Path(\"..\") / \"data\" / \"Raw_data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_path = output_dir / \"monthlychallenges.csv\"\n",
    "pd.DataFrame(rows, columns=[\n",
    "    \"title\", \"description\", \"start_date\", \"end_date\", \"goal_minutes\", \"reward_item_id\"\n",
    "]).to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"\\nCSV saved.\")\n",
    "\n",
    "\n",
    "# INSERT ROW BY ROW SAFE MODE\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO monthlychallenges\n",
    "(title, description, start_date, end_date, goal_minutes, reward_item_id)\n",
    "VALUES (%s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "inserted = 0\n",
    "failed = 0\n",
    "\n",
    "for r in rows:\n",
    "    try:\n",
    "        cursor.execute(insert_query, r)\n",
    "        inserted += 1\n",
    "    except Exception as e:\n",
    "        print(\"\\n FAILED ROW:\", r)\n",
    "        print(\"MYSQL ERROR:\", e)\n",
    "        failed += 1\n",
    "\n",
    "connection.commit()\n",
    "cursor.close()\n",
    "connection.close()\n",
    "\n",
    "print(f\"\\nInserted {inserted} rows successfully.\")\n",
    "print(f\"Failed rows: {failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f988affd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 users and 100 challenges.\n",
      "\n",
      "Generated 100 unique user-challenge-progress rows.\n",
      "âœ” Saved CSV â†’ ../data/Raw_data/userchallengeprogress.csv\n",
      "\n",
      "âœ” Inserted 100 rows successfully.\n",
      " Failed inserts: 0\n"
     ]
    }
   ],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Connect to DB\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Fetch foreign keys\n",
    "cursor.execute(\"SELECT user_id FROM Users;\")\n",
    "user_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.execute(\"SELECT challenge_id, goal_minutes FROM monthlychallenges;\")\n",
    "challenges = cursor.fetchall()  # list of (challenge_id, goal_minutes)\n",
    "\n",
    "print(f\"Loaded {len(user_ids)} users and {len(challenges)} challenges.\")\n",
    "\n",
    "rows = []\n",
    "used_pairs = set()  # to enforce PRIMARY KEY(user_id, challenge_id)\n",
    "\n",
    "# Generate exactly 100 unique user-challenge progress logs\n",
    "while len(rows) < 100:\n",
    "\n",
    "    user_id = random.choice(user_ids)\n",
    "    challenge_id, goal_minutes = random.choice(challenges)\n",
    "\n",
    "    pair = (user_id, challenge_id)\n",
    "    if pair in used_pairs:\n",
    "        continue  # avoid violating composite PK\n",
    "\n",
    "    used_pairs.add(pair)\n",
    "\n",
    "    # Progress\n",
    "    total_minutes = random.randint(0, goal_minutes + 200)\n",
    "\n",
    "    is_completed = total_minutes >= goal_minutes\n",
    "\n",
    "    # Completion date only if completed\n",
    "    if is_completed:\n",
    "        completed_at = fake.date_time_between(start_date=\"-30d\", end_date=\"now\")\n",
    "    else:\n",
    "        completed_at = None\n",
    "\n",
    "    rows.append((\n",
    "        user_id,\n",
    "        challenge_id,\n",
    "        total_minutes,\n",
    "        is_completed,\n",
    "        completed_at\n",
    "    ))\n",
    "\n",
    "print(f\"\\nGenerated {len(rows)} unique user-challenge-progress rows.\")\n",
    "\n",
    "\n",
    "# Save CSV â†’ ../data/Raw_data/userchallengeprogress.csv\n",
    "output_dir = Path(\"..\") / \"data\" / \"Raw_data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_path = output_dir / \"userchallengeprogress.csv\"\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    \"user_id\", \"challenge_id\", \"total_minutes\",\n",
    "    \"is_completed\", \"completed_at\"\n",
    "])\n",
    "\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"âœ” Saved CSV â†’ {csv_path}\")\n",
    "\n",
    "\n",
    "# Insert into MySQL row-by-row (safe)\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO userchallengeprogress\n",
    "(user_id, challenge_id, total_minutes, is_completed, completed_at)\n",
    "VALUES (%s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "inserted = 0\n",
    "failed = 0\n",
    "\n",
    "for r in rows:\n",
    "    try:\n",
    "        cursor.execute(insert_query, r)\n",
    "        inserted += 1\n",
    "    except Exception as e:\n",
    "        print(\"\\n Failed row:\", r)\n",
    "        print(\"Error:\", e)\n",
    "        failed += 1\n",
    "\n",
    "connection.commit()\n",
    "cursor.close()\n",
    "connection.close()\n",
    "\n",
    "print(f\"\\nâœ” Inserted {inserted} rows successfully.\")\n",
    "print(f\" Failed inserts: {failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd3ffb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 100 users, 100 timer sessions, 5 mood levels.\n",
      "\n",
      "Generated 100 mood entries.\n",
      "âœ” Saved moodtracking.csv with 100 rows.\n",
      "\n",
      "âœ” Successfully inserted 100 rows into moodtracking.\n",
      " Failed inserts: 0\n"
     ]
    }
   ],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Connect to DB\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "\n",
    "# FETCH FOREIGN KEYS\n",
    "cursor.execute(\"SELECT user_id FROM Users;\")\n",
    "user_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.execute(\"SELECT timer_id FROM timersessions;\")\n",
    "timer_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.execute(\"SELECT mood_level_id FROM mood_levels;\")\n",
    "mood_levels = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(f\"Loaded: {len(user_ids)} users, {len(timer_ids)} timer sessions, {len(mood_levels)} mood levels.\")\n",
    "\n",
    "# GENERATE 100 MOOD ENTRIES\n",
    "rows = []\n",
    "\n",
    "for _ in range(100):\n",
    "\n",
    "    user_id = random.choice(user_ids)\n",
    "\n",
    "    # 70% chance mood is linked to a timer session, 30% chance standalone\n",
    "    timer_id = random.choice(timer_ids) if random.random() < 0.7 else None\n",
    "\n",
    "    mood_level_id = random.choice(mood_levels)\n",
    "\n",
    "    # Sometimes user writes a short note\n",
    "    note = fake.sentence(nb_words=random.randint(4, 12))\n",
    "    note = note[:250]  # ensure <255 chars for safety\n",
    "\n",
    "    # Recorded at: random time within last 30 days\n",
    "    recorded_at = fake.date_time_between(start_date=\"-30d\", end_date=\"now\")\n",
    "\n",
    "    rows.append((\n",
    "        user_id,\n",
    "        timer_id,\n",
    "        mood_level_id,\n",
    "        note,\n",
    "        recorded_at\n",
    "    ))\n",
    "\n",
    "print(f\"\\nGenerated {len(rows)} mood entries.\")\n",
    "\n",
    "# SAVE CSV\n",
    "output_dir = Path(\"..\") / \"data\" / \"Raw_data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_path = output_dir / \"moodtracking.csv\"\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    \"user_id\", \"timer_id\", \"mood_level_id\", \"note\", \"recorded_at\"\n",
    "])\n",
    "\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"âœ” Saved moodtracking.csv with {len(df)} rows.\")\n",
    "\n",
    "\n",
    "# INSERT INTO MySQL â€” row-by-row for safety\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO moodtracking\n",
    "(user_id, timer_id, mood_level_id, note, recorded_at)\n",
    "VALUES (%s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "inserted = 0\n",
    "failed = 0\n",
    "\n",
    "for r in rows:\n",
    "    try:\n",
    "        cursor.execute(insert_query, r)\n",
    "        inserted += 1\n",
    "    except Exception as e:\n",
    "        print(\"\\n Failed row:\", r)\n",
    "        print(\"Error:\", e)\n",
    "        failed += 1\n",
    "\n",
    "connection.commit()\n",
    "cursor.close()\n",
    "connection.close()\n",
    "\n",
    "print(f\"\\nâœ” Successfully inserted {inserted} rows into moodtracking.\")\n",
    "print(f\" Failed inserts: {failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c1ac311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 users.\n",
      "\n",
      "Generated 400 leaderboard entries.\n",
      "âœ” Saved leaderboardstats.csv with 400 rows.\n",
      "\n",
      "âœ” Inserted 400 leaderboard rows successfully.\n",
      " Failed inserts: 0\n"
     ]
    }
   ],
   "source": [
    "from db.connect_db import get_connection\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Connect\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# FETCH USERS\n",
    "cursor.execute(\"SELECT user_id FROM Users;\")\n",
    "user_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(f\"Loaded {len(user_ids)} users.\")\n",
    "\n",
    "# Leaderboard periods\n",
    "periods = [\"daily\", \"weekly\", \"monthly\", \"all_time\"]\n",
    "\n",
    "rows = []\n",
    "\n",
    "\n",
    "# GENERATE LEADERBOARD STATS\n",
    "for user_id in user_ids:\n",
    "\n",
    "    # Base performance for all_time\n",
    "    base_total_focus = random.randint(500, 8000)\n",
    "    base_total_sessions = base_total_focus // random.randint(20, 60)\n",
    "    base_streak = random.randint(1, 40)\n",
    "\n",
    "    # Create entries for each period\n",
    "    for period in periods:\n",
    "\n",
    "        if period == \"daily\":\n",
    "            total_focus = random.randint(0, 240)                      # up to 4 hours\n",
    "            total_sessions = max(1, total_focus // random.randint(20, 45))\n",
    "            streak = random.randint(0, 5)\n",
    "\n",
    "        elif period == \"weekly\":\n",
    "            total_focus = random.randint(200, 1200)\n",
    "            total_sessions = max(1, total_focus // random.randint(20, 45))\n",
    "            streak = random.randint(1, 15)\n",
    "\n",
    "        elif period == \"monthly\":\n",
    "            total_focus = random.randint(800, 5000)\n",
    "            total_sessions = max(1, total_focus // random.randint(20, 45))\n",
    "            streak = random.randint(5, 30)\n",
    "\n",
    "        else:  # all_time\n",
    "            total_focus = base_total_focus\n",
    "            total_sessions = base_total_sessions\n",
    "            streak = base_streak\n",
    "\n",
    "        rows.append((\n",
    "            user_id,\n",
    "            total_focus,\n",
    "            total_sessions,\n",
    "            streak,\n",
    "            period,\n",
    "            datetime.now()\n",
    "        ))\n",
    "\n",
    "print(f\"\\nGenerated {len(rows)} leaderboard entries.\")\n",
    "\n",
    "\n",
    "# SAVE CSV\n",
    "output_dir = Path(\"..\") / \"data\" / \"Raw_data\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_path = output_dir / \"leaderboardstats.csv\"\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    \"user_id\", \"total_focus_min\", \"total_sessions\",\n",
    "    \"streak_days\", \"period_type\", \"updated_at\"\n",
    "])\n",
    "\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"âœ” Saved leaderboardstats.csv with {len(df)} rows.\")\n",
    "\n",
    "\n",
    "# INSERT INTO DATABASE\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO leaderboardstats\n",
    "(user_id, total_focus_min, total_sessions, streak_days, period_type, updated_at)\n",
    "VALUES (%s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "inserted = 0\n",
    "failed = 0\n",
    "\n",
    "for r in rows:\n",
    "    try:\n",
    "        cursor.execute(insert_query, r)\n",
    "        inserted += 1\n",
    "    except Exception as e:\n",
    "        print(\"\\n Failed row:\", r)\n",
    "        print(\"Error:\", e)\n",
    "        failed += 1\n",
    "\n",
    "connection.commit()\n",
    "cursor.close()\n",
    "connection.close()\n",
    "\n",
    "print(f\"\\nâœ” Inserted {inserted} leaderboard rows successfully.\")\n",
    "print(f\" Failed inserts: {failed}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
